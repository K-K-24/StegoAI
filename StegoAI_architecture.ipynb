{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "McxCeceOrCvz",
        "outputId": "50748f2d-135c-4d5f-a1a6-8ce5ccba238c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.13.0\n",
            "  Downloading tensorflow-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (524.1 MB)\n",
            "\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.8/524.1 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:08\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a848a97006e5>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Step 1: Mount Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mdataset_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/My Drive/Dataset/S_Cover'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    131\u001b[0m   )\n\u001b[1;32m    132\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# !pip install --upgrade tensorflow==2.13.0\n",
        "\n",
        "\n",
        "# from google.colab import drive\n",
        "# import os\n",
        "# import numpy as np\n",
        "# import cv2\n",
        "# !pip install pandas\n",
        "\n",
        "# from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "# from tensorflow.keras.applications import VGG16\n",
        "# from tensorflow.keras.models import Model\n",
        "\n",
        "# # Step 1: Mount Google Drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# dataset_dir = '/content/drive/My Drive/Dataset/S_Cover'\n",
        "\n",
        "# # Function to preprocess and augment image\n",
        "# def preprocess_image(img):\n",
        "#     img = cv2.resize(img, (224, 224))  # Resize image\n",
        "#     img = img.astype('float32') / 255.0  # Normalize pixel values to [0, 1]\n",
        "#     # Add additional preprocessing steps if needed\n",
        "#     return img\n",
        "\n",
        "# # Function to extract color distribution\n",
        "# def extract_color_distribution(img):\n",
        "#     hist = cv2.calcHist([img], [0, 1, 2], None, [256, 256, 256], [0, 256, 0, 256, 0, 256]).flatten()\n",
        "#     return hist\n",
        "\n",
        "# # Function to extract texture patterns\n",
        "# def extract_texture_patterns(img):\n",
        "#     gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "#     radius = 3\n",
        "#     num_points = 8 * radius\n",
        "#     lbp = cv2.LBP(gray_img, num_points, radius, method='uniform')\n",
        "#     return lbp.flatten()\n",
        "\n",
        "# # Function to extract edge detection\n",
        "# def extract_edge_detection(img):\n",
        "#     gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "#     edges = cv2.Canny(gray_img, 100, 200)\n",
        "#     return edges.flatten()\n",
        "\n",
        "# # Function to extract standard deviation\n",
        "# def extract_std_deviation(img):\n",
        "#     gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "#     std_dev = np.std(gray_img)\n",
        "#     return std_dev\n",
        "\n",
        "# # Function to extract color depth\n",
        "# def extract_color_depth(img):\n",
        "#     unique_colors = np.unique(img.reshape(-1, img.shape[2]), axis=0)\n",
        "#     color_depth = len(unique_colors)\n",
        "#     return color_depth\n",
        "\n",
        "# # Step 2: Load and Preprocess Dataset\n",
        "\n",
        "# # Step 3: Create Data Generator for Augmentation\n",
        "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# datagen = ImageDataGenerator(\n",
        "#     rotation_range=20,\n",
        "#     width_shift_range=0.2,\n",
        "#     height_shift_range=0.2,\n",
        "#     shear_range=0.2,\n",
        "#     zoom_range=0.2,\n",
        "#     horizontal_flip=True,\n",
        "#     fill_mode='nearest'\n",
        "# )\n",
        "\n",
        "# # Step 4: Create Feature Extraction Model (VGG16)\n",
        "# base_model = VGG16(weights='imagenet', include_top=False)\n",
        "# feature_extraction_model = Model(inputs=base_model.input, outputs=base_model.get_layer('block5_pool').output)\n",
        "\n",
        "# # Step 5: Extract Features\n",
        "# def extract_features(img):\n",
        "#     img_array = img_to_array(img)\n",
        "#     img_array = np.expand_dims(img_array, axis=0)\n",
        "#     img_array = preprocess_image(img_array[0])  # Preprocess image\n",
        "\n",
        "#     # Extract features using VGG16 model\n",
        "#     vgg_features = feature_extraction_model.predict(np.expand_dims(img_array, axis=0)).flatten()\n",
        "\n",
        "#     # Extract additional properties\n",
        "#     color_distribution = extract_color_distribution(img_array[0])\n",
        "#     texture_patterns = extract_texture_patterns(img_array)\n",
        "#     edge_detection = extract_edge_detection(img_array)\n",
        "#     std_deviation = extract_std_deviation(img_array)\n",
        "#     color_depth = extract_color_depth(img_array)\n",
        "\n",
        "#     return np.concatenate([vgg_features, color_distribution, texture_patterns, edge_detection, [std_deviation], [color_depth]])\n",
        "\n",
        "\n",
        "# features_list = []\n",
        "\n",
        "# for class_name in os.listdir(dataset_dir):\n",
        "#     class_path = os.path.join(dataset_dir, class_name)\n",
        "#     if os.path.isdir(class_path):\n",
        "#         for img_name in os.listdir(class_path):\n",
        "#             img_path = os.path.join(class_path, img_name)\n",
        "#             img = cv2.imread(img_path)\n",
        "\n",
        "#             # Apply data augmentation\n",
        "#             for batch in datagen.flow(np.expand_dims(img, axis=0), batch_size=1):\n",
        "#                 augmented_img = batch[0]\n",
        "#                 features = extract_features(augmented_img)\n",
        "#                 features_list.append(features)\n",
        "#                 break  # Only need one augmented image\n",
        "\n",
        "# # Step 6: Save Features\n",
        "# np.save('features.npy', np.array(features_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TfCiRCpfPbU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bxWr2x4Wv07",
        "outputId": "c73e007e-92e0-40e7-8395-21146a66393e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: tensorflow\n",
            "Version: 2.15.0\n",
            "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
            "Home-page: https://www.tensorflow.org/\n",
            "Author: Google Inc.\n",
            "Author-email: packages@tensorflow.org\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: absl-py, astunparse, flatbuffers, gast, google-pasta, grpcio, h5py, keras, libclang, ml-dtypes, numpy, opt-einsum, packaging, protobuf, setuptools, six, tensorboard, tensorflow-estimator, tensorflow-io-gcs-filesystem, termcolor, typing-extensions, wrapt\n",
            "Required-by: dopamine-rl, tf_keras\n"
          ]
        }
      ],
      "source": [
        "pip show tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZ3hdqzUrNyO",
        "outputId": "0a7ce90c-2fef-4312-c660-a26d9a51dcf6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of features array: (0,)\n",
            "First few features:\n",
            "[]\n"
          ]
        }
      ],
      "source": [
        "# # Load saved features\n",
        "# features = np.load('features.npy')\n",
        "\n",
        "# # Display the shape of the features array\n",
        "# print(\"Shape of features array:\", features.shape)\n",
        "\n",
        "# # Display the first few features\n",
        "# print(\"First few features:\")\n",
        "# print(features[:5])  # Adjust the number to display more or fewer features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "PSkw0DAKuAAB",
        "outputId": "666e6315-5822-43a3-efdb-ce2f8c1a511e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/My Drive/Dataset/S_Training'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-1f95025ec17a>\u001b[0m in \u001b[0;36m<cell line: 55>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mfeatures_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mimg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Processing image:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Debugging: Print image path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/Dataset/S_Training'"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# import os\n",
        "# import numpy as np\n",
        "# from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "# from tensorflow.keras.applications import VGG16\n",
        "# from tensorflow.keras.models import Model\n",
        "\n",
        "# # Step 1: Mount Google Drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# # Step 2: Define dataset directory\n",
        "# dataset_dir = '/content/drive/My Drive/Dataset/S_Training'\n",
        "\n",
        "# # Define preprocessing functions and feature extraction functions (same as your provided code)\n",
        "\n",
        "# # Step 3: Create Data Generator for Augmentation\n",
        "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# datagen = ImageDataGenerator(\n",
        "#     rotation_range=20,\n",
        "#     width_shift_range=0.2,\n",
        "#     height_shift_range=0.2,\n",
        "#     shear_range=0.2,\n",
        "#     zoom_range=0.2,\n",
        "#     horizontal_flip=True,\n",
        "#     fill_mode='nearest'\n",
        "# )\n",
        "\n",
        "# # Step 4: Create Feature Extraction Model (VGG16)\n",
        "# base_model = VGG16(weights='imagenet', include_top=False)\n",
        "# feature_extraction_model = Model(inputs=base_model.input, outputs=base_model.get_layer('block5_pool').output)\n",
        "\n",
        "# # Step 5: Extract Features\n",
        "# def extract_features(img):\n",
        "#     img_array = img_to_array(img)\n",
        "#     img_array = np.expand_dims(img_array, axis=0)\n",
        "#     img_array = preprocess_image(img_array[0])  # Preprocess image\n",
        "#     print(\"Image dimensions:\", img.shape)  # Debugging: Print image dimensions\n",
        "\n",
        "#     # Extract features using VGG16 model\n",
        "#     vgg_features = feature_extraction_model.predict(np.expand_dims(img_array, axis=0)).flatten()\n",
        "\n",
        "#     # Extract additional properties\n",
        "#     # color_distribution = extract_color_distribution(img_array[0])\n",
        "#     texture_patterns = extract_texture_patterns(img_array)\n",
        "#     edge_detection = extract_edge_detection(img_array)\n",
        "#     std_deviation = extract_std_deviation(img_array)\n",
        "#     color_depth = extract_color_depth(img_array)\n",
        "\n",
        "#     return np.concatenate([vgg_features, color_distribution, texture_patterns, edge_detection, [std_deviation], [color_depth]])\n",
        "\n",
        "# # Step 6: Iterate through images, apply data augmentation, and extract features\n",
        "# features_list = []\n",
        "\n",
        "# for img_name in os.listdir(dataset_dir):\n",
        "#     img_path = os.path.join(dataset_dir, img_name)\n",
        "#     print(\"Processing image:\", img_path)  # Debugging: Print image path\n",
        "#     img = load_img(img_path, target_size=(224, 224))  # Load image\n",
        "\n",
        "#     img = img_to_array(img)\n",
        "\n",
        "#     # Apply data augmentation\n",
        "#     for batch in datagen.flow(np.expand_dims(img, axis=0), batch_size=1):\n",
        "#         augmented_img = batch[0]\n",
        "#         print(\"Augmented image dimensions:\", augmented_img.shape)  # Debugging: Print augmented image dimensions\n",
        "#         features = extract_features(augmented_img)\n",
        "#         features_list.append(features)\n",
        "#         break  # Only need one augmented image\n",
        "\n",
        "# # Step 7: Save Features\n",
        "# np.save('features.npy', np.array(features_list))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTe2Su8JwCBP",
        "outputId": "7d1beaae-9c2f-4f32-f8e1-cde9ebda7339"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==2.13.0 in /usr/local/lib/python3.10/dist-packages (2.13.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (24.3.7)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.62.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (3.9.0)\n",
            "Requirement already satisfied: keras<2.14,>=2.13.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (2.13.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (16.0.6)\n",
            "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.24.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.14,>=2.13 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (2.13.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (2.13.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (0.36.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.13.0) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (1.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.2.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.24.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.24.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 0s 0us/step\n",
            "1/1 [==============================] - 1s 743ms/step\n",
            "1/1 [==============================] - 1s 563ms/step\n",
            "1/1 [==============================] - 1s 969ms/step\n",
            "1/1 [==============================] - 1s 915ms/step\n",
            "1/1 [==============================] - 1s 544ms/step\n",
            "1/1 [==============================] - 1s 591ms/step\n",
            "1/1 [==============================] - 1s 552ms/step\n",
            "1/1 [==============================] - 1s 555ms/step\n",
            "1/1 [==============================] - 1s 532ms/step\n",
            "1/1 [==============================] - 1s 534ms/step\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade tensorflow==2.13.0\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "import numpy as np\n",
        "!pip install opencv-python\n",
        "import cv2\n",
        "!pip install pandas\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Step 1: Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "dataset_dir = '/content/drive/My Drive/Dataset/S_Cover'\n",
        "\n",
        "# Function to preprocess and augment image\n",
        "def preprocess_image(img):\n",
        "    img = cv2.resize(img, (224, 224))  # Resize image\n",
        "    img = img.astype('float32') / 255.0  # Normalize pixel values to [0, 1]\n",
        "    # Add additional preprocessing steps if needed\n",
        "    return img\n",
        "\n",
        "\n",
        "def extract_mean_pixel_value(img):\n",
        "    # Compute mean pixel value\n",
        "    mean_pixel_value = np.mean(img)\n",
        "    return mean_pixel_value\n",
        "\n",
        "def extract_hvs_features(img):\n",
        "    # Convert image to HSV color space\n",
        "    hsv_img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "    # Compute average hue, saturation, and value\n",
        "    avg_hue = np.mean(hsv_img[:, :, 0])\n",
        "    avg_saturation = np.mean(hsv_img[:, :, 1])\n",
        "    avg_value = np.mean(hsv_img[:, :, 2])\n",
        "\n",
        "    # Compute dominant hue using color histogram\n",
        "    hue_hist = cv2.calcHist([hsv_img], [0], None, [180], [0, 180])\n",
        "    dominant_hue = np.argmax(hue_hist)\n",
        "\n",
        "    return avg_hue, avg_saturation, avg_value, dominant_hue\n",
        "\n",
        "def extract_brightness(img):\n",
        "    # Convert image to grayscale\n",
        "    gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    # Compute average brightness\n",
        "    brightness = np.mean(gray_img)\n",
        "    return brightness\n",
        "\n",
        "# Function to extract edge detection\n",
        "def extract_edge_detection(img):\n",
        "    gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    gray_img = gray_img.astype(np.uint8)\n",
        "    edges = cv2.Canny(gray_img, 100, 200)\n",
        "    return edges.flatten()\n",
        "\n",
        "# Function to extract standard deviation\n",
        "def extract_std_deviation(img):\n",
        "    gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    std_dev = np.std(gray_img)\n",
        "    return std_dev\n",
        "\n",
        "# Function to extract color depth\n",
        "def extract_color_depth(img):\n",
        "    unique_colors = np.unique(img.reshape(-1, img.shape[2]), axis=0)\n",
        "    color_depth = len(unique_colors)\n",
        "    return color_depth\n",
        "\n",
        "# Step 2: Load and Preprocess Dataset\n",
        "\n",
        "# Step 3: Create Data Generator for Augmentation\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Step 4: Create Feature Extraction Model (VGG16)\n",
        "base_model = VGG16(weights='imagenet', include_top=False)\n",
        "feature_extraction_model = Model(inputs=base_model.input, outputs=base_model.get_layer('block5_pool').output)\n",
        "\n",
        "# Step 5: Extract Features\n",
        "def extract_features(img):\n",
        "    img_array = img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    img_array = preprocess_image(img_array[0])  # Preprocess image\n",
        "\n",
        "    # Extract features using VGG16 model\n",
        "    vgg_features = feature_extraction_model.predict(np.expand_dims(img_array, axis=0)).flatten()\n",
        "\n",
        "    # Extract additional properties\n",
        "    # color_distribution = extract_color_distribution(img_array[0])\n",
        "    # texture_patterns = extract_texture_patterns(img_array)\n",
        "    edge_detection = extract_edge_detection(img_array)\n",
        "    std_deviation = extract_std_deviation(img_array)\n",
        "    color_depth = extract_color_depth(img_array)\n",
        "    brightness = extract_brightness(img_array)\n",
        "    avg_hue, avg_saturation, avg_value, dominant_hue = extract_hvs_features(img_array)\n",
        "    mean_pixel_value = extract_mean_pixel_value(img_array)\n",
        "\n",
        "        # Create a dictionary to store features\n",
        "    features_dict = {\n",
        "        # 'vgg_features': vgg_features,\n",
        "        # 'edge_detection': edge_detection,\n",
        "        'std_deviation': std_deviation,\n",
        "        'color_depth': color_depth,\n",
        "        'brightness': brightness,\n",
        "        'avg_hue': avg_hue,\n",
        "        'avg_saturation': avg_saturation,\n",
        "        'avg_value': avg_value,\n",
        "        'dominant_hue': dominant_hue,\n",
        "        'mean_pixel_value':mean_pixel_value\n",
        "    }\n",
        "\n",
        "    return features_dict\n",
        "\n",
        "    # return np.concatenate([vgg_features,edge_detection, [std_deviation], [color_depth],[brightness],[avg_hue],[avg_saturation],[avg_value],[dominant_hue],[mean_pixel_value]])\n",
        "\n",
        "# Step 6: Iterate through images, apply data augmentation, and extract features\n",
        "# features_list = []\n",
        "\n",
        "# for img_name in os.listdir(dataset_dir):\n",
        "#     img_path = os.path.join(dataset_dir, img_name)\n",
        "#     img = load_img(img_path, target_size=(224, 224))  # Load image\n",
        "#     img = img_to_array(img)\n",
        "\n",
        "#     # Apply data augmentation\n",
        "#     for batch in datagen.flow(np.expand_dims(img, axis=0), batch_size=1):\n",
        "#         augmented_img = batch[0]\n",
        "#         features = extract_features(augmented_img)\n",
        "#         features_list.append(features)\n",
        "#         break  # Only need one augmented image\n",
        "\n",
        "# Step 7: Save Features\n",
        "# np.save('features.npy', np.array(features_list))\n",
        "\n",
        "\n",
        "\n",
        "# Step 6: Iterate through images, apply data augmentation, and extract features\n",
        "features_dict_list = []\n",
        "\n",
        "for img_name in os.listdir(dataset_dir):\n",
        "    img_path = os.path.join(dataset_dir, img_name)\n",
        "    img = load_img(img_path, target_size=(224, 224))  # Load image\n",
        "    img = img_to_array(img)\n",
        "\n",
        "\n",
        "    # Apply data augmentation\n",
        "    for batch in datagen.flow(np.expand_dims(img, axis=0), batch_size=1):\n",
        "        augmented_img = batch[0]\n",
        "        features_dict = extract_features(augmented_img)\n",
        "        features_dict_list.append(features_dict)\n",
        "        break  # Only need one augmented image\n",
        "\n",
        "# Step 7: Save Features\n",
        "np.save('features.npy', features_dict_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UQbBDuqBi7b"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import Input, Dense, Concatenate, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkFdb4v8BtJ7",
        "outputId": "8746521b-8fc3-4b31-a62b-11a2a153813b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/12\n",
            "1/1 [==============================] - 1s 820ms/step - loss: 0.5459 - val_loss: 0.5391\n",
            "Epoch 2/12\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 0.5459 - val_loss: 0.5391\n",
            "Epoch 3/12\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.5459 - val_loss: 0.5391\n",
            "Epoch 4/12\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 0.5459 - val_loss: 0.5391\n",
            "Epoch 5/12\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.5459 - val_loss: 0.5391\n",
            "Epoch 6/12\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.5459 - val_loss: 0.5391\n",
            "Epoch 7/12\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.5459 - val_loss: 0.5391\n",
            "Epoch 8/12\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.5459 - val_loss: 0.5391\n",
            "Epoch 9/12\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.5459 - val_loss: 0.5391\n",
            "Epoch 10/12\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.5459 - val_loss: 0.5391\n",
            "Epoch 11/12\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.5459 - val_loss: 0.5391\n",
            "Epoch 12/12\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.5459 - val_loss: 0.5391\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7c8b0351bac0>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define custom contrastive loss function\n",
        "def contrastive_loss(y_true, y_pred, margin=1):\n",
        "    squared_pred = tf.square(y_pred)\n",
        "    squared_margin = tf.square(tf.maximum(margin - y_pred, 0))\n",
        "    loss = y_true * squared_pred + (1 - y_true) * squared_margin\n",
        "    return tf.reduce_mean(loss)\n",
        "\n",
        "# Register the custom loss function\n",
        "tf.keras.utils.get_custom_objects()['contrastive_loss'] = contrastive_loss\n",
        "\n",
        "# Load features extracted from images\n",
        "features_dict_list = np.load('features.npy', allow_pickle=True)\n",
        "\n",
        "# Resize or pad features to a fixed shape\n",
        "# max_shape = max(features_dict['vgg_features'].shape[0] for features_dict in features_dict_list)\n",
        "resized_features = []\n",
        "\n",
        "for features_dict in features_dict_list:\n",
        "    # vgg_features = features_dict['vgg_features']\n",
        "    # edge_detection = features_dict['edge_detection']\n",
        "    std_deviation = features_dict['std_deviation']\n",
        "    color_depth = features_dict['color_depth']\n",
        "    brightness = features_dict['brightness']\n",
        "    avg_hue = features_dict['avg_hue']\n",
        "    avg_saturation = features_dict['avg_saturation']\n",
        "    avg_value = features_dict['avg_value']\n",
        "    dominant_hue = features_dict['dominant_hue']\n",
        "    mean_pixel_value = features_dict['mean_pixel_value']\n",
        "\n",
        "    # Resize or pad features to match max_shape\n",
        "    # vgg_features_resized = np.pad(vgg_features, ((0, max_shape - vgg_features.shape[0]), (0, 0)), mode='constant')\n",
        "    # edge_detection_resized = np.pad(edge_detection, (0, max_shape - edge_detection.shape[0]), mode='constant')\n",
        "    std_deviation_resized = np.array([std_deviation])\n",
        "    color_depth_resized = np.array([color_depth])\n",
        "    brightness_resized = np.array([brightness])\n",
        "    avg_hue_resized = np.array([avg_hue])\n",
        "    avg_saturation_resized = np.array([avg_saturation])\n",
        "    avg_value_resized = np.array([avg_value])\n",
        "    dominant_hue_resized = np.array([dominant_hue])\n",
        "    mean_pixel_value_resized = np.array([mean_pixel_value])\n",
        "\n",
        "    # Combine resized features into a single array\n",
        "\n",
        "    concatenated_features = np.concatenate([std_deviation_resized, color_depth_resized,\n",
        "                                            brightness_resized, avg_hue_resized,\n",
        "                                            avg_saturation_resized, avg_value_resized,\n",
        "                                            dominant_hue_resized, mean_pixel_value_resized])\n",
        "\n",
        "    resized_features.append(concatenated_features)\n",
        "\n",
        "# Combine all features into one array\n",
        "features = np.array(resized_features)\n",
        "\n",
        "\n",
        "# Define input shape\n",
        "input_shape = features.shape[1:]\n",
        "\n",
        "# Split features into training and validation sets\n",
        "X_train, X_val = train_test_split(features, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define a contrastive learning model\n",
        "def contrastive_model(input_shape):\n",
        "    input_1 = Input(shape=input_shape)\n",
        "    input_2 = Input(shape=input_shape)\n",
        "\n",
        "    # Flatten inputs\n",
        "    flatten = Flatten()\n",
        "\n",
        "    # Concatenate features\n",
        "    concatenated_features = Concatenate()([flatten(input_1), flatten(input_2)])\n",
        "\n",
        "    # Output layer\n",
        "    output = Dense(128, activation='relu')(concatenated_features)\n",
        "\n",
        "    # Create the model\n",
        "    model = Model(inputs=[input_1, input_2], outputs=output)\n",
        "\n",
        "    return model\n",
        "# Instantiate the contrastive model\n",
        "c_model = contrastive_model(input_shape)\n",
        "\n",
        "# Compile the model\n",
        "c_model.compile(optimizer=Adam(), loss='contrastive_loss')  # Define contrastive loss function\n",
        "\n",
        "# Train the model\n",
        "# Generate dummy target data for contrastive loss\n",
        "\n",
        "y_train_dummy = np.zeros(len(X_train))  # Assuming all pairs are dissimilar\n",
        "y_val_dummy = np.zeros(len(X_val))\n",
        "\n",
        "# Train the model\n",
        "c_model.fit([X_train, X_train], y=y_train_dummy, epochs=12, batch_size=60, validation_data=([X_val, X_val], y_val_dummy))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dweVg8iig-u",
        "outputId": "3c4bf17f-c931-4504-cd37-8cf42f4488c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: tensorflow\n",
            "Version: 2.13.0\n",
            "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
            "Home-page: https://www.tensorflow.org/\n",
            "Author: Google Inc.\n",
            "Author-email: packages@tensorflow.org\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: absl-py, astunparse, flatbuffers, gast, google-pasta, grpcio, h5py, keras, libclang, numpy, opt-einsum, packaging, protobuf, setuptools, six, tensorboard, tensorflow-estimator, tensorflow-io-gcs-filesystem, termcolor, typing-extensions, wrapt\n",
            "Required-by: dopamine-rl, tf_keras\n"
          ]
        }
      ],
      "source": [
        "pip show tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXlSBjUlFial",
        "outputId": "cca3a5fb-c0bd-400b-e050-deb74cf57c82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the path to save the trained model\n",
        "model_path = '/content/drive/My Drive/Model/c_model.h5'\n",
        "\n",
        "# Save the trained model\n",
        "c_model.save(model_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "OetopD8HSxWD",
        "outputId": "e1e102ec-d8dc-4f44-8326-38f1f619fb72"
      },
      "outputs": [
        {
          "ename": "OSError",
          "evalue": "No file or directory found at /content/drive/My Drive/Model/c_model.h5",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-b3940ebcffcb>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Load the trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mc_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;31m# Legacy case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m     return legacy_sm_saving_lib.load_model(\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/legacy/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    232\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                             raise IOError(\n\u001b[0m\u001b[1;32m    235\u001b[0m                                 \u001b[0;34mf\"No file or directory found at {filepath_str}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                             )\n",
            "\u001b[0;31mOSError\u001b[0m: No file or directory found at /content/drive/My Drive/Model/c_model.h5"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Define the path to the saved model\n",
        "model_path = '/content/drive/My Drive/Model/c_model.h5'\n",
        "\n",
        "# Load the trained model\n",
        "c_model = load_model(model_path, compile=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "XuwhQdYgb72q",
        "outputId": "6a0da717-21b4-4488-d29b-532f25076412"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'os' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-e365c2346378>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Iterate over each image in the dataset directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mimg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
          ]
        }
      ],
      "source": [
        "# Path to the directory containing the dataset images\n",
        "dataset_dir = '/content/drive/My Drive/Dataset/S_Cover'\n",
        "\n",
        "# List to store the embeddings for each image\n",
        "embeddings_list = []\n",
        "\n",
        "# Iterate over each image in the dataset directory\n",
        "for img_name in os.listdir(dataset_dir):\n",
        "    img_path = os.path.join(dataset_dir, img_name)\n",
        "\n",
        "    # Load the image\n",
        "    img = load_img(img_path, target_size=(224, 224))\n",
        "    img_array = img_to_array(img)\n",
        "\n",
        "    # Extract features from the image\n",
        "    img_features = extract_features(img_array)\n",
        "\n",
        "    # Convert the features into a numpy array\n",
        "    img_features_array = np.array([list(img_features.values())])\n",
        "\n",
        "    # Generate embeddings for the image using the contrastive model\n",
        "    embeddings = c_model.predict([img_features_array, img_features_array])\n",
        "\n",
        "    print(embeddings.shape())\n",
        "\n",
        "    # Append the embeddings to the list\n",
        "    embeddings_list.append(embeddings)\n",
        "\n",
        "\n",
        "\n",
        "# Convert the list of embeddings into a numpy array\n",
        "# embeddings_array = np.array(embeddings_list)\n",
        "\n",
        "# Save the array to a file\n",
        "# np.save(\"embeddings_array.npy\", embeddings_array)\n",
        "\n",
        "# print(embeddings_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68_9SGaYPTG3",
        "outputId": "cca56543-7a71-40da-a37e-eafb6c00ba7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n",
            "arrange_proctoring\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 128)\n",
            "(1, 128)\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import string\n",
        "\n",
        "import random\n",
        "import gensim.downloader as api\n",
        "\n",
        "# Load the Word2Vec model\n",
        "w2v_model = api.load('word2vec-google-news-300')\n",
        "\n",
        "# Access the model's vocabulary\n",
        "# For gensim versions 4.0.0 and above, use `key_to_index`\n",
        "vocab_list = list(w2v_model.key_to_index.keys())\n",
        "\n",
        "# For older gensim versions, you might need to use `vocab` instead:\n",
        "# vocab_list = list(w2v_model.vocab.keys())\n",
        "\n",
        "# Generate a 5000 membered array with words from the model's vocabulary\n",
        "words = [random.choice(vocab_list) for _ in range(5)]\n",
        "print(words[0])\n",
        "\n",
        "# Display the first 10 entries to verify\n",
        "# print(words_from_vocab[:10])\n",
        "\n",
        "\n",
        "# print(words)\n",
        "\n",
        "# Display the first 10 entries to verify\n",
        "# random_texts[:10]\n",
        "\n",
        "import gensim.downloader as api\n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "\n",
        "!pip install nltk\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "# Load pre-trained Word2Vec model\n",
        "w2v_model = api.load('word2vec-google-news-300')\n",
        "\n",
        "# Function to convert text to embedding\n",
        "def text_to_embedding(text, embedding_size=128):\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Initialize an empty list to store word vectors\n",
        "    word_vectors = []\n",
        "\n",
        "    # Iterate over tokens and get their word vectors\n",
        "    for token in tokens:\n",
        "        try:\n",
        "            # Get the word vector for the token\n",
        "            word_vector = w2v_model[token]\n",
        "            word_vectors.append(word_vector)\n",
        "        except KeyError:\n",
        "            # Ignore tokens not found in the vocabulary\n",
        "            pass\n",
        "\n",
        "    # Average the word vectors to get the text embedding\n",
        "    if word_vectors:\n",
        "        text_embedding = np.mean(word_vectors, axis=0)\n",
        "    else:\n",
        "        # If no word vectors found, use zeros\n",
        "        text_embedding = np.zeros((embedding_size,))\n",
        "\n",
        "    # Pad or truncate the embedding to have the desired size\n",
        "    if len(text_embedding) < embedding_size:\n",
        "        text_embedding = np.pad(text_embedding, ((0, embedding_size - len(text_embedding))), mode='constant')\n",
        "    elif len(text_embedding) > embedding_size:\n",
        "        text_embedding = text_embedding[:embedding_size]\n",
        "\n",
        "    # Reshape to match the desired shape (1, 128)\n",
        "    text_embedding = text_embedding.reshape((1, embedding_size))\n",
        "\n",
        "    return text_embedding\n",
        "\n",
        "# Example usage\n",
        "text = \"Your text here\"\n",
        "text_embedding = text_to_embedding(text)\n",
        "print(text_embedding.shape)  # Output: (1, 128)\n",
        "\n",
        "\n",
        "# Words to generate embeddings for\n",
        "# words = [\"hello\", \"world\", \"suresh\", \"ramesh\", \"power\", \"affirmations\", \"30days\", \"meditation\", \"universe\", \"faith\"]\n",
        "\n",
        "# Generate embeddings for each word\n",
        "embeddings = []\n",
        "for word in words:\n",
        "    embedding = text_to_embedding(word)\n",
        "    embeddings.append(embedding)\n",
        "print(embeddings[0].shape)\n",
        "\n",
        "# # Convert embeddings to array format\n",
        "embeddings_array = np.array(embeddings)\n",
        "\n",
        "\n",
        "# print(embeddings)\n",
        "\n",
        "# # Convert the list of embeddings into a numpy array\n",
        "# embeddings_array = np.array(embeddings)\n",
        "\n",
        "# Save the array to a file\n",
        "np.save(\"embeddings_array.npy\", embeddings_array)\n",
        "\n",
        "\n",
        "# Convert embeddings to array format\n",
        "# text_embeddings = np.concatenate(embeddings, axis=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "# Step 1: Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def jpg_to_png(input_folder, output_folder):\n",
        "    # Create output folder if it doesn't exist\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    # Loop through all files in the input folder\n",
        "    for filename in os.listdir(input_folder):\n",
        "        if filename.endswith(\".jpg\"):\n",
        "            # Open the image file\n",
        "            with Image.open(os.path.join(input_folder, filename)) as img:\n",
        "                # Construct the output file path with .png extension\n",
        "                output_file = os.path.splitext(filename)[0] + \".png\"\n",
        "                # Save the image as PNG format in the output folder\n",
        "                img.save(os.path.join(output_folder, output_file), \"PNG\")\n",
        "                print(f\"Converted {filename} to {output_file}\")\n",
        "\n",
        "# Change these paths to your input and output folders\n",
        "input_folder = '/content/drive/My Drive/Dataset/just/'\n",
        "output_folder = \"/content/drive/My Drive/Dataset/png1/\"\n",
        "\n",
        "jpg_to_png(input_folder, output_folder)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9IPUady-647",
        "outputId": "9482a09c-d09d-4703-c168-c061eb52eb05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "5198\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDKfMVFqlaEG",
        "outputId": "6f77a3cb-d898-4016-cb9a-0049faaf9960"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "!pip install Pillow\n",
        "\n",
        "# Step 1: Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "def load_and_preprocess_image(image_path):\n",
        "    img = cv2.imread(image_path)\n",
        "    img = cv2.resize(img, (224, 224))  # Resize image\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
        "    img = img.astype('float32') / 255.0  # Normalize pixel values to [0, 1]\n",
        "    return img\n",
        "\n",
        "def encode_text_into_image(preprocessed_img, text, output_path):\n",
        "    encoded_img = Image.fromarray((preprocessed_img * 255).astype('uint8'))  # Convert back to PIL image\n",
        "    binary_text = ''.join(format(ord(char), '08b') for char in text)\n",
        "    binary_text += '1111111111111110'  # Delimiter\n",
        "\n",
        "    data_index = 0\n",
        "    for row in range(encoded_img.height):\n",
        "        for col in range(encoded_img.width):\n",
        "            if data_index < len(binary_text):\n",
        "                pixel = list(encoded_img.getpixel((col, row)))\n",
        "                for n in range(3):\n",
        "                    if data_index < len(binary_text):\n",
        "                        pixel[n] = pixel[n] & ~1 | int(binary_text[data_index])\n",
        "                        data_index += 1\n",
        "                    else:\n",
        "                        break\n",
        "                encoded_img.putpixel((col, row), tuple(pixel))\n",
        "\n",
        "    encoded_img.save(output_path, format='PNG')\n",
        "\n",
        "def decode_text_from_image(image_path):\n",
        "    img = Image.open(image_path)\n",
        "    binary_text = ''\n",
        "    for row in range(img.height):\n",
        "        for col in range(img.width):\n",
        "            pixel = img.getpixel((col, row))\n",
        "            for n in pixel[:3]:\n",
        "                binary_text += str(n & 1)\n",
        "\n",
        "    delimiter_index = binary_text.find('1111111111111110')\n",
        "    if delimiter_index != -1:\n",
        "        binary_text = binary_text[:delimiter_index]\n",
        "\n",
        "    decoded_text = ''.join([chr(int(binary_text[i:i+8], 2)) for i in range(0, len(binary_text), 8)])\n",
        "\n",
        "    return decoded_text\n",
        "\n",
        "# Directory paths and words\n",
        "image_directory = '/content/drive/My Drive/Dataset/png1/'\n",
        "output_directory = '/content/drive/My Drive/Dataset/embedded1/'\n",
        "# words = ['Word #1', 'Word #2', ..., 'Word #50']\n",
        "\n",
        "# Ensure output directory exists\n",
        "if not os.path.exists(output_directory):\n",
        "    os.makedirs(output_directory)\n",
        "\n",
        "# Process each image\n",
        "for i, image_name in enumerate(os.listdir(image_directory)):\n",
        "    if image_name.endswith('.png'):\n",
        "        image_path = os.path.join(image_directory, image_name)\n",
        "        preprocessed_img = load_and_preprocess_image(image_path)\n",
        "\n",
        "        output_path = os.path.join(output_directory, f\"encoded_{image_name}\")\n",
        "        encode_text_into_image(preprocessed_img, words[i], output_path)\n",
        "\n",
        "        # Decode for demonstration (optional)\n",
        "        # decoded_text = decode_text_from_image(output_path)\n",
        "        # print(f\"Decoded text from {output_path}: {decoded_text}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7SNhf4oFiC_C"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dropout,ZeroPadding2D,Dense, Reshape, Flatten, Concatenate, Input, Conv2D, Conv2DTranspose, LeakyReLU, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from tensorflow.keras.utils import plot_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1z7U8mIixbgS"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Input, Concatenate, Conv2D, Conv2DTranspose, Flatten, Dense, LeakyReLU, ZeroPadding2D, Reshape\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "# Define the GAN architecture\n",
        "# Generator\n",
        "def build_generator(input_shape, embedding_shape):\n",
        "    input_img = Input(shape=input_shape)  # Input for the original images\n",
        "    input_embedding = Input(shape=embedding_shape)  # Input for the embeddings\n",
        "\n",
        "    # Reshape embedding input to match the shape of the original image\n",
        "    embedding_reshaped = Reshape((1, 1, embedding_shape[-1]))(input_embedding)\n",
        "\n",
        "    # Pad the embedding to match the spatial dimensions of the image input\n",
        "    padding_height = input_shape[0] - 1\n",
        "    padding_width = input_shape[1] - 1\n",
        "    padding = ZeroPadding2D(padding=((0, padding_height), (0, padding_width)))(embedding_reshaped)\n",
        "\n",
        "    # Concatenate original image and embedding\n",
        "    concatenated = Concatenate(axis=-1)([input_img, padding])\n",
        "\n",
        "    # # Generator network architecture\n",
        "    # x = Conv2D(64, kernel_size=3, strides=2, padding='same', activation='relu')(concatenated)\n",
        "    # x = Conv2D(128, kernel_size=3, strides=2, padding='same', activation='relu')(x)\n",
        "    # x = Conv2DTranspose(128, kernel_size=3, strides=2, padding='same', activation='relu')(x)\n",
        "    # x = Conv2DTranspose(64, kernel_size=3, strides=2, padding='same', activation='relu')(x)\n",
        "    # generated_img = Conv2DTranspose(3, kernel_size=3, strides=1, padding='same', activation='sigmoid')(x)  # Output image\n",
        "\n",
        "\n",
        "    x = Conv2D(64, kernel_size=3, strides=2, padding='same')(concatenated)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "\n",
        "    x = Conv2D(128, kernel_size=3, strides=2, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "\n",
        "    x = Conv2DTranspose(128, kernel_size=3, strides=2, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "    x = Conv2DTranspose(64, kernel_size=3, strides=2, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "    generated_img = Conv2DTranspose(3, kernel_size=3, strides=1, padding='same', activation='sigmoid')(x)\n",
        "\n",
        "    # Model\n",
        "    generator = Model(inputs=[input_img, input_embedding], outputs=generated_img)\n",
        "    return generator\n",
        "\n",
        "# Discriminator\n",
        "def build_discriminator(input_shape, embedding_shape):\n",
        "    input_img = Input(shape=input_shape)  # Input for images with embedded text\n",
        "    input_embedding = Input(shape=embedding_shape)  # Input for embeddings\n",
        "\n",
        "        # Reshape embedding input to match the shape of the original image\n",
        "    embedding_reshaped = Reshape((1, 1, embedding_shape[-1]))(input_embedding)\n",
        "\n",
        "    # Pad the embedding to match the spatial dimensions of the image input\n",
        "    padding_height = input_shape[0] - 1\n",
        "    padding_width = input_shape[1] - 1\n",
        "    padding = ZeroPadding2D(padding=((0, padding_height), (0, padding_width)))(embedding_reshaped)\n",
        "\n",
        "    # Concatenate image and embedding\n",
        "    concatenated = Concatenate(axis=-1)([input_img, padding])\n",
        "\n",
        "    x = Conv2D(64, kernel_size=3, strides=2, padding='same')(concatenated)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "\n",
        "    x = Conv2D(128, kernel_size=3, strides=2, padding='same')(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "\n",
        "    # Discriminator network architecture\n",
        "    # x = Conv2D(64, kernel_size=3, strides=2, padding='same', activation=LeakyReLU(alpha=0.2))(concatenated)\n",
        "    # x = Conv2D(128, kernel_size=3, strides=2, padding='same', activation=LeakyReLU(alpha=0.2))(x)\n",
        "    x = Flatten()(x)\n",
        "    validity = Dense(1, activation='sigmoid')(x)  # Output validity\n",
        "\n",
        "    # Model\n",
        "    discriminator = Model(inputs=[input_img, input_embedding], outputs=validity)\n",
        "    return discriminator\n",
        "\n",
        "# Adversarial Model\n",
        "def build_adversarial_model(generator, discriminator, input_shape, embedding_shape):\n",
        "    input_img = Input(shape=input_shape)  # Input for original images\n",
        "    input_embedding = Input(shape=embedding_shape)  # Input for embeddings\n",
        "\n",
        "    # Generate fake images\n",
        "    generated_img = generator([input_img, input_embedding])\n",
        "\n",
        "    # Disable discriminator training during generator training\n",
        "    discriminator.trainable = False\n",
        "\n",
        "    # Validity of generated images\n",
        "    validity = discriminator([generated_img, input_embedding])\n",
        "\n",
        "    # Adversarial model\n",
        "    adversarial_model = Model(inputs=[input_img, input_embedding], outputs=validity)\n",
        "    return adversarial_model\n",
        "\n",
        "embedding_shape = (1, 128)  # Assuming text embeddings are of shape (1, 128)\n",
        "input_shape = (224, 224, 3)\n",
        "\n",
        "# Build and compile discriminator\n",
        "discriminator = build_discriminator(input_shape, embedding_shape)\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
        "\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "\n",
        "# Define learning rate schedule\n",
        "initial_learning_rate = 0.0001\n",
        "lr_schedule = ExponentialDecay(\n",
        "    initial_learning_rate=initial_learning_rate,\n",
        "    decay_steps=1000,\n",
        "    decay_rate=0.96,\n",
        "    staircase=True)\n",
        "\n",
        "# Optimizers with learning rate schedule\n",
        "optimizer_gen = tf.keras.optimizers.Adam(learning_rate=lr_schedule, beta_1=0.5)\n",
        "optimizer_disc = tf.keras.optimizers.Adam(learning_rate=lr_schedule, beta_1=0.5)\n",
        "\n",
        "# Build generator\n",
        "generator = build_generator(input_shape, embedding_shape)\n",
        "\n",
        "\n",
        "# Build adversarial model\n",
        "adversarial_model = build_adversarial_model(generator, discriminator, input_shape, embedding_shape)\n",
        "adversarial_model.compile(optimizer=optimizer_gen, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Compile discriminator (separately for fine-tuning)\n",
        "discriminator.compile(optimizer=optimizer_disc, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Compile generator (separately for fine-tuning)\n",
        "generator.compile(optimizer=optimizer_gen, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Assuming TensorFlow is imported as tf\n",
        "# import numpy as np\n",
        "\n",
        "# # Dummy data\n",
        "# dummy_images = np.random.rand(1, 224, 224, 3).astype('float32')  # Single image\n",
        "# dummy_embeddings = np.random.rand(1, 128).astype('float32')  # Corresponding embedding\n",
        "\n",
        "# # Adjust embeddings shape\n",
        "# dummy_embeddings = np.expand_dims(dummy_embeddings, axis=1)  # Now has shape (1, 1, 128)\n",
        "\n",
        "# # Test discriminator\n",
        "# d_result = discriminator.predict([dummy_images, dummy_embeddings])\n",
        "# print(f\"Discriminator output shape: {d_result.shape}, output: {d_result}\")\n",
        "\n",
        "# # Test generator\n",
        "# g_result = generator.predict([dummy_images, dummy_embeddings])\n",
        "# print(f\"Generator output shape: {g_result.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vV_0HEyzyiIz",
        "outputId": "efd853cf-3b3e-4fed-9116-9f0e2b0b5bab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 142ms/step\n",
            "Discriminator output shape: (1, 1), output: [[0.4783205]]\n",
            "1/1 [==============================] - 0s 290ms/step\n",
            "Generator output shape: (1, 224, 224, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0cJePZ1FZC9"
      },
      "outputs": [],
      "source": [
        "input_images = \"/content/drive/My Drive/Dataset/png1\"\n",
        "embedded_images = \"/content/drive/My Drive/Dataset/embedded1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7vY0FckFscW",
        "outputId": "15a38b2d-4646-409f-e3b8-0d91f890c6d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 128)\n",
            "Epoch 1/1\n",
            "(224, 224, 3)\n",
            "1/1 [==============================] - 1s 719ms/step\n",
            "Step: 0/5, D_loss: [8.5721691 0.5      ], G_loss: [0.7290518879890442, 0.0], D_accuracy: 0.5, G_accuracy: 0.0\n",
            "1/1 [==============================] - 1s 599ms/step\n",
            "1/1 [==============================] - 0s 493ms/step\n",
            "(224, 224, 3)\n",
            "1/1 [==============================] - 1s 532ms/step\n",
            "Step: 1/5, D_loss: [3.38574174 0.5       ], G_loss: [0.735383152961731, 0.0], D_accuracy: 0.5, G_accuracy: 0.0\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "(224, 224, 3)\n",
            "1/1 [==============================] - 0s 135ms/step\n",
            "Step: 2/5, D_loss: [11.08906323  0.5       ], G_loss: [0.7350376844406128, 0.0], D_accuracy: 0.5, G_accuracy: 0.0\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "(224, 224, 3)\n",
            "1/1 [==============================] - 0s 160ms/step\n",
            "Step: 3/5, D_loss: [17.58435404  0.5       ], G_loss: [0.7233566641807556, 0.0], D_accuracy: 0.5, G_accuracy: 0.0\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "(224, 224, 3)\n",
            "1/1 [==============================] - 0s 166ms/step\n",
            "Step: 4/5, D_loss: [1.72777307 0.5       ], G_loss: [0.7197744846343994, 0.0], D_accuracy: 0.5, G_accuracy: 0.0\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "Confusion Matrix:\n",
            "[[0 5]\n",
            " [5 0]]\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# Function to load and preprocess images\n",
        "def load_and_preprocess_image(image_path):\n",
        "    img = cv2.imread(image_path)\n",
        "    img = cv2.resize(img, (224, 224))  # Resize image\n",
        "    # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
        "    # img = img.astype('float32') / 255.0  # Normalize pixel values to [0, 1]\n",
        "    return img\n",
        "\n",
        "def calculate_precision_recall(conf_matrix):\n",
        "    # Extract true positives, false positives, and false negatives from the confusion matrix\n",
        "    TP = conf_matrix[1, 1]\n",
        "    FP = conf_matrix[0, 1]\n",
        "    FN = conf_matrix[1, 0]\n",
        "\n",
        "    # Calculate precision\n",
        "    precision = TP / (TP + FP)\n",
        "\n",
        "    # Calculate recall\n",
        "    recall = TP / (TP + FN)\n",
        "\n",
        "    return precision, recall\n",
        "\n",
        "# Load embeddings of real images without hidden content\n",
        "embeddings_array = np.load('embeddings_array.npy')\n",
        "print(embeddings_array[0].shape)\n",
        "# print(embeddings_array)\n",
        "# Define batch size\n",
        "batch_size = 1\n",
        "# print(len(embeddings_array))\n",
        "# Define number of training steps\n",
        "num_steps = len(embeddings_array) // batch_size\n",
        "# print(num_steps)\n",
        "\n",
        "# Define number of epochs\n",
        "epochs = 1  # Adjusted number of epochs\n",
        "\n",
        "# Initialize variables to accumulate predictions and true labels\n",
        "all_predictions = []\n",
        "all_true_labels = []\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "\n",
        "    # Training loop\n",
        "    for step in range(num_steps):\n",
        "        # Prepare batch of real images without hidden content and their corresponding embeddings\n",
        "        batch_embeddings = embeddings_array[step * batch_size: (step + 1) * batch_size]\n",
        "        # Adjust embeddings shape\n",
        "        # batch_embeddings = np.expand_dims(batch_embeddings, axis=1)  # Now has shape (1, 1, 128)\n",
        "        batch_images = []\n",
        "        for img_name in os.listdir(input_images)[step * batch_size: (step + 1) * batch_size]:\n",
        "            img_path = os.path.join(input_images, img_name)\n",
        "            img = load_and_preprocess_image(img_path)\n",
        "            batch_images.append(img)\n",
        "        batch_images = np.array(batch_images)\n",
        "\n",
        "        # Prepare batch of embedded images and their corresponding embeddings\n",
        "        batch_embedded_images = []\n",
        "        for img_name in os.listdir(embedded_images)[step * batch_size: (step + 1) * batch_size]:\n",
        "            img_path = os.path.join(embedded_images, img_name)\n",
        "            img = load_and_preprocess_image(img_path)\n",
        "            # img = cv2.imread(image_path)\n",
        "            batch_embedded_images.append(img)\n",
        "        print(batch_embedded_images[0].shape)\n",
        "        batch_embedded_images = np.array(batch_embedded_images)\n",
        "\n",
        "        # Train the discriminator\n",
        "        d_loss_real = discriminator.train_on_batch([batch_embedded_images, batch_embeddings], tf.ones((batch_size,1)))  # Real images should be labeled as 1\n",
        "        fake_images = generator.predict([batch_images, batch_embeddings])  # Generate fake images using real embeddings\n",
        "        d_loss_fake = discriminator.train_on_batch([fake_images, batch_embeddings], tf.zeros((batch_size,1)))  # Fake images should be labeled as 0\n",
        "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "        # Train the generator (adversarial model)\n",
        "        g_loss = adversarial_model.train_on_batch([batch_images, batch_embeddings], np.ones((batch_size, 1)))  # Fool the discriminator\n",
        "\n",
        "        # Calculate discriminator accuracy\n",
        "        discriminator_accuracy_real = d_loss_real[1]\n",
        "        discriminator_accuracy_fake = d_loss_fake[1]\n",
        "        discriminator_accuracy = 0.5 * (discriminator_accuracy_real + discriminator_accuracy_fake)\n",
        "\n",
        "        # Calculate generator accuracy\n",
        "        generator_accuracy = g_loss[1]\n",
        "\n",
        "        # Print progress\n",
        "        print(f\"Step: {step}/{num_steps}, D_loss: {d_loss}, G_loss: {g_loss}, D_accuracy: {discriminator_accuracy}, G_accuracy: {generator_accuracy}\")\n",
        "\n",
        "        # Get discriminator predictions for real and fake images\n",
        "        discriminator_predictions_real = discriminator.predict([batch_embedded_images, batch_embeddings])\n",
        "        discriminator_predictions_fake = discriminator.predict([fake_images, batch_embeddings])\n",
        "\n",
        "        # Threshold discriminator predictions\n",
        "        discriminator_predictions_real = (discriminator_predictions_real >= 0.5).astype(int)\n",
        "        discriminator_predictions_fake = (discriminator_predictions_fake < 0.5).astype(int)\n",
        "\n",
        "        # Accumulate predictions and true labels\n",
        "        all_predictions.extend(discriminator_predictions_real)\n",
        "        all_predictions.extend(discriminator_predictions_fake)\n",
        "        all_true_labels.extend([1] * batch_size)\n",
        "        all_true_labels.extend([0] * batch_size)\n",
        "\n",
        "    conf_matrix = confusion_matrix(all_true_labels, all_predictions)\n",
        "\n",
        "    # Print confusion matrix\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(conf_matrix)\n",
        "\n",
        "    # Calculate precision and recall\n",
        "    precision, recall = calculate_precision_recall(conf_matrix)\n",
        "\n",
        "    # Print precision and recall\n",
        "    print(f\"Precision: {precision}\")\n",
        "    print(f\"Recall: {recall}\")\n",
        "\n",
        "# Save the trained models if needed\n",
        "discriminator.save('discriminator.h5')\n",
        "generator.save('generator.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('generator.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "jl3O_TvQl-Eb",
        "outputId": "fe6266c1-9cdb-4d9f-97b6-bc7bba59c0f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_155a3698-3dce-4448-a512-b319b69c3265\", \"generator.h5\", 4580304)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout,Reshape\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, TimeDistributed, Lambda, RepeatVector\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "# Load VGG16 as the base model, excluding the top (classification) layer\n",
        "base_model = VGG16(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
        "\n",
        "# Set the layers of the base model to be non-trainable to preserve pre-trained features\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Add new layers on top of the base model\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)  # Reduces the features to a 1D vector to prevent overfitting\n",
        "\n",
        "# Optional: Add a Dropout layer for regularization\n",
        "x = Dropout(0.5)(x)\n",
        "\n",
        "# Add a Dense layer to allow the network to learn complex patterns\n",
        "x = Dense(512, activation='relu')(x)\n",
        "\n",
        "# Final Dense layer with output shape (128,) to match text embeddings\n",
        "final_output = Dense(128, activation='relu')(x)\n",
        "\n",
        "\n",
        "# Reshape the output to (None, 1, 128)\n",
        "final_output_reshaped = Reshape((1, 128))(final_output)\n",
        "\n",
        "# Define the new model, from base model input to the new output\n",
        "encoder_model = Model(inputs=base_model.input, outputs=final_output_reshaped)\n",
        "\n",
        "# Display the model summary to verify the architecture\n",
        "# encoder_model.summary()\n",
        "\n",
        "\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, RepeatVector,TimeDistributed\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "vocab_size = 4500  # Adjust based on your dataset\n",
        "max_seq_length = 20  # Maximum length of text sequences\n",
        "embedding_dim = 128  # Dimension of the embeddings\n",
        "\n",
        "# Decoder Model\n",
        "decoder_inputs = Input(shape=(1, embedding_dim))  # Adjusted to match embedding shape\n",
        "\n",
        "# Repeat the encoder's output to match the desired sequence length\n",
        "# decoder_repeated = RepeatVector(max_seq_length)(decoder_inputs)\n",
        "\n",
        "# LSTM layer\n",
        "# decoder_lstm = LSTM(256, return_sequences=True)(decoder_inputs)\n",
        "\n",
        "# Repeat the input for the sequence length to match the LSTM input requirements\n",
        "repeated_decoder_inputs = RepeatVector(max_seq_length)(Lambda(lambda x: K.squeeze(x, axis=1))(decoder_inputs))\n",
        "\n",
        "# LSTM layer\n",
        "decoder_lstm = LSTM(256, return_sequences=True, name=\"decoder_lstm\")(repeated_decoder_inputs)\n",
        "\n",
        "# Use TimeDistributed to apply Dense layer to each time step independently\n",
        "decoder_time_dist = TimeDistributed(Dense(vocab_size, activation='softmax'))(decoder_lstm)\n",
        "\n",
        "# Building the model with TimeDistributed Dense layer\n",
        "decoder_model = Model(inputs=decoder_inputs, outputs=decoder_time_dist)\n",
        "\n",
        "# # Predicting each word in the sequence\n",
        "# decoder_dense = Dense(vocab_size, activation='softmax')\n",
        "# decoder_outputs = decoder_dense(decoder_lstm)\n",
        "\n",
        "# # Building the model\n",
        "# decoder_model = Model(inputs=decoder_inputs, outputs=decoder_outputs)\n",
        "\n",
        "# Model compilation\n",
        "decoder_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Display the model architecture\n",
        "# decoder_model.summary()\n",
        "\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "\n",
        "\n",
        "# Define the full model that chains encoder and decoder\n",
        "def build_end_to_end_model(encoder, decoder):\n",
        "    # Define the input for the full model, which is the input for the encoder\n",
        "    full_model_input = Input(shape=(224, 224, 3), name='full_model_input')\n",
        "\n",
        "    # Get the encoder output (embeddings) for the given input\n",
        "    encoder_output = encoder(full_model_input)\n",
        "\n",
        "    # Feed the encoder output (embeddings) as input to the decoder\n",
        "    decoder_output = decoder(encoder_output)\n",
        "\n",
        "    print(decoder_output.shape)\n",
        "\n",
        "    # Define the full end-to-end model\n",
        "    full_model = Model(inputs=full_model_input, outputs=decoder_output, name='end_to_end_model')\n",
        "\n",
        "    return full_model\n",
        "\n",
        "# Build the full model\n",
        "end_to_end_model = build_end_to_end_model(encoder_model, decoder_model)\n",
        "\n",
        "# Model compilation (adjust optimizer and loss as needed)\n",
        "end_to_end_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Display the model summary to verify the architecture\n",
        "end_to_end_model.summary()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHfefxgLj_5p",
        "outputId": "f5f0fafc-0cd1-4d6c-c691-af5d19a3bf1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 20, 50)\n",
            "Model: \"end_to_end_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " full_model_input (InputLay  [(None, 224, 224, 3)]     0         \n",
            " er)                                                             \n",
            "                                                                 \n",
            " model_11 (Functional)       (None, 1, 128)            15043008  \n",
            "                                                                 \n",
            " model_12 (Functional)       (None, 20, 50)            407090    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 15450098 (58.94 MB)\n",
            "Trainable params: 735410 (2.81 MB)\n",
            "Non-trainable params: 14714688 (56.13 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(words[1])\n",
        "\n",
        "from google.colab import drive\n",
        "# !pip install Pillow\n",
        "\n",
        "# Step 1: Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dz3wLhQv0lTH",
        "outputId": "0c5a6acc-c5a8-4da4-c66e-b9b9680f6a35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
        "\n",
        "\n",
        "# def preprocess_text(texts, tokenizer, max_seq_length):\n",
        "#     sequences = tokenizer.texts_to_sequences(texts)\n",
        "#     padded_sequences = pad_sequences(sequences, maxlen=max_seq_length, padding='post')\n",
        "#     return padded_sequences\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size)\n",
        "# Fit your tokenizer here on your text data\n",
        "# print(words)\n",
        "\n",
        "tokenizer.fit_on_texts(words)\n",
        "\n",
        "def convert_to_classes(predictions):\n",
        "    return np.argmax(predictions, axis=-1)\n",
        "\n",
        "# Initialize lists to store precision and recall\n",
        "epoch_precisions = []\n",
        "epoch_recalls = []\n",
        "\n",
        "text_sequences = words\n",
        "\n",
        "\n",
        "# Directory containing preprocessed images\n",
        "image_directory = '/content/drive/My Drive/Dataset/embedded1/'\n",
        "loaded_images = []\n",
        "\n",
        "# Iterate through each file in the directory\n",
        "for filename in os.listdir(image_directory):\n",
        "    if filename.endswith(\".jpg\") or filename.endswith(\".png\"):  # Check for image files\n",
        "        # Construct the full path to the image file\n",
        "        file_path = os.path.join(image_directory, filename)\n",
        "\n",
        "        # Load and preprocess the image\n",
        "        img = cv2.imread(file_path)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        img = img / 255.0  # Normalize image pixels to [0, 1]\n",
        "        loaded_images.append(img)\n",
        "\n",
        "images_array = np.array(loaded_images)\n",
        "\n",
        "text_sequences = tokenizer.texts_to_sequences(text_sequences)\n",
        "target_data = pad_sequences(text_sequences, maxlen=max_seq_length, padding='post')\n",
        "num_samples = len(loaded_images)\n",
        "# One-hot encode your sequences. This might be memory-intensive.\n",
        "target_data_one_hot = np.zeros((num_samples, max_seq_length, vocab_size), dtype='float32')\n",
        "\n",
        "for i, seq in enumerate(target_data):\n",
        "    for j, word_index in enumerate(seq):\n",
        "        if word_index != 0:  # Assuming 0 is used for padding\n",
        "            target_data_one_hot[i, j, word_index] = 1.0\n",
        "\n",
        "# print(images_array.shape)  # Should be (num_samples, 224, 224, 3)\n",
        "print(target_data_one_hot.shape)  # Should be (num_samples, max_seq_length)\n",
        "\n",
        "\n",
        "\n",
        "# Train the model\n",
        "\n",
        "end_to_end_model.fit(images_array, target_data_one_hot, batch_size=32, epochs=10, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMuebhcgfElx",
        "outputId": "6fc8ad86-46fd-4e3f-8da6-53d56729f69c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50, 20, 50)\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 38s 13s/step - loss: 0.2000 - accuracy: 0.0030\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 28s 10s/step - loss: 0.1986 - accuracy: 0.0020\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 29s 11s/step - loss: 0.1971 - accuracy: 0.0030\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 28s 11s/step - loss: 0.1974 - accuracy: 0.0030\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 28s 10s/step - loss: 0.1981 - accuracy: 0.0010\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 27s 10s/step - loss: 0.1982 - accuracy: 0.0010\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 28s 11s/step - loss: 0.1932 - accuracy: 0.0020\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 29s 10s/step - loss: 0.1925 - accuracy: 0.0020\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 27s 10s/step - loss: 0.1930 - accuracy: 0.0020\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 27s 9s/step - loss: 0.1911 - accuracy: 0.0030\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ca91c057bb0>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# def preprocess_text(texts, tokenizer, max_seq_length):\n",
        "#     sequences = tokenizer.texts_to_sequences(texts)\n",
        "#     padded_sequences = pad_sequences(sequences, maxlen=max_seq_length, padding='post')\n",
        "#     return padded_sequences\n",
        "\n",
        "# Assuming you have a tokenizer fitted on your text data\n",
        "tokenizer = Tokenizer(num_words=vocab_size)\n",
        "# Fit your tokenizer here on your text data\n",
        "# print(words)\n",
        "\n",
        "tokenizer.fit_on_texts(words)\n",
        "\n",
        "text_sequences = words\n",
        "\n",
        "\n",
        "# Directory containing preprocessed images\n",
        "image_directory = '/content/drive/My Drive/Dataset/embedded1/'\n",
        "loaded_images = []\n",
        "\n",
        "# Iterate through each file in the directory\n",
        "for filename in os.listdir(image_directory):\n",
        "    if filename.endswith(\".jpg\") or filename.endswith(\".png\"):  # Check for image files\n",
        "        # Construct the full path to the image file\n",
        "        file_path = os.path.join(image_directory, filename)\n",
        "\n",
        "        # Load and preprocess the image\n",
        "        img = cv2.imread(file_path)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        img = img / 255.0  # Normalize image pixels to [0, 1]\n",
        "        loaded_images.append(img)\n",
        "\n",
        "images_array = np.array(loaded_images)\n",
        "\n",
        "\n",
        "\n",
        "# text_sequences = tokenizer.texts_to_sequences(text_sequences)\n",
        "target_data = pad_sequences(text_sequences, maxlen=max_seq_length, padding='post')\n",
        "num_samples = len(loaded_images)\n",
        "# One-hot encode your sequences. This might be memory-intensive.\n",
        "target_data_one_hot = np.zeros((num_samples, max_seq_length, vocab_size), dtype='float32')\n",
        "\n",
        "for i, seq in enumerate(target_data):\n",
        "    for j, word_index in enumerate(seq):\n",
        "        if word_index != 0:  # Assuming 0 is used for padding\n",
        "            target_data_one_hot[i, j, word_index] = 1.0\n",
        "\n",
        "# print(images_array.shape)  # Should be (num_samples, 224, 224, 3)\n",
        "print(target_data_one_hot.shape)  # Should be (num_samples, max_seq_length)\n",
        "\n",
        "# print(len(loaded_images))\n",
        "\n",
        "# print(len(words))\n",
        "\n",
        "# Preprocess texts\n",
        "# text_sequences = preprocess_text(words, tokenizer, max_seq_length)\n",
        "\n",
        "# target_data = np.array(text_sequences)\n",
        "\n",
        "\n",
        "# Train the model\n",
        "# Note: Adjust the `batch_size` and `epochs` based on your dataset and training needs\n",
        "end_to_end_model.fit(images_array, target_data_one_hot, batch_size=32, epochs=10, verbose=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OeShF21q0VYQ",
        "outputId": "dd8102ce-c543-4686-e8af-4d9dfb32c323"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50, 20, 52)\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "Graph execution error:\n\nDetected at node gradient_tape/categorical_crossentropy/mul/BroadcastGradientArgs defined at (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"<ipython-input-29-c4233da864d5>\", line 69, in <cell line: 69>\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1807, in fit\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1154, in train_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\", line 543, in minimize\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\", line 276, in compute_gradients\n\nIncompatible shapes: [32,20,52] vs. [32,20,50]\n\t [[{{node gradient_tape/categorical_crossentropy/mul/BroadcastGradientArgs}}]] [Op:__inference_train_function_16736]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-c4233da864d5>\u001b[0m in \u001b[0;36m<cell line: 69>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;31m# Note: Adjust the `batch_size` and `epochs` based on your dataset and training needs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0mend_to_end_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_data_one_hot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node gradient_tape/categorical_crossentropy/mul/BroadcastGradientArgs defined at (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"<ipython-input-29-c4233da864d5>\", line 69, in <cell line: 69>\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1807, in fit\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1154, in train_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\", line 543, in minimize\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\", line 276, in compute_gradients\n\nIncompatible shapes: [32,20,52] vs. [32,20,50]\n\t [[{{node gradient_tape/categorical_crossentropy/mul/BroadcastGradientArgs}}]] [Op:__inference_train_function_16736]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def prepare_image(file_path):\n",
        "    img = cv2.imread(file_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    img = cv2.resize(img, (224, 224))\n",
        "    img = img / 255.0\n",
        "    img = np.expand_dims(img, axis=0)  # Expand dims to add the batch size\n",
        "    return img\n",
        "\n",
        "# image_path = \"path_to_your_image.jpg\"\n",
        "\n",
        "image_path = \"/content/drive/My Drive/Dataset/embedded1/encoded_Copy of 00952.png\"\n",
        "prepared_image = prepare_image(image_path)\n",
        "# img = cv2.imread(file_path)\n",
        "# print(prepared_image.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOHb-pqk0kSv",
        "outputId": "0847a351-0697-4542-97d7-89d4dd81130e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 224, 224, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sequence(sequence, tokenizer):\n",
        "    word_index = tokenizer.word_index\n",
        "    index_word = {index: word for word, index in word_index.items()}\n",
        "    return ' '.join([index_word.get(i, '?') for i in sequence])\n",
        "\n",
        "predictions = end_to_end_model.predict(prepared_image)\n",
        "predicted_sequence = np.argmax(predictions, axis=-1)[0]  # Assuming batch size of 1\n",
        "decoded_prediction = decode_sequence(predicted_sequence, tokenizer)\n",
        "print(decoded_prediction)\n",
        "\n",
        "for i in words:\n",
        "  print(i)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMa-3UVGb5NE",
        "outputId": "35f8842e-d0ea-4e3f-ae9e-99f446283d92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "andrea senior hardjapamekas williams williams williams williams williams williams williams williams williams williams williams williams williams williams williams williams williams\n",
            "Oellerich\n",
            "chunky_guacamole\n",
            "MEOH\n",
            "FRANdata\n",
            "DURHAM_NC_Shelden_Williams\n",
            "Nativa\n",
            "Shannon_Liss_Riordan\n",
            "Ctesiphon\n",
            "NYSE_HNR\n",
            "Hovitz\n",
            "Kilopass\n",
            "Andrea_Prasow_senior\n",
            "ganglionic\n",
            "sixteenth_pole_Shoemaker\n",
            "Erry_Riyana_Hardjapamekas\n",
            "Eschelman\n",
            "Ketu\n",
            "Thomas_Winterhoff\n",
            "Bihar_Nitish_Kumar\n",
            "IRA_Rollover\n",
            "Kinnerton\n",
            "Thomas_Timlen\n",
            "MICROSOFT_WINDOWS\n",
            "multi_instrumentalist\n",
            "CAMILLE\n",
            "Bryton_McClure\n",
            "Werdelin\n",
            "Poly_Implant\n",
            "copay\n",
            "Macroeconomic_Research\n",
            "Gourdine_thanked\n",
            "bottomfishing\n",
            "Informal_Ministerial_Meeting\n",
            "Tehila\n",
            "Mezzaroba\n",
            "interceding\n",
            "Oklawaha_River\n",
            "LEFTERIS_PITARAKIS\n",
            "Iyin\n",
            "Andrea_Horban\n",
            "Fatah_Tanzim\n",
            "VoipNow\n",
            "Final_Fantasy_XII\n",
            "etition\n",
            "Haji_Abdul_Razak\n",
            "Ilene_Dillon\n",
            "snom_VoIP\n",
            "TSX_VENTURE_TMD\n",
            "chakkajam\n",
            "Greg_Gostanian\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xYV1RQXk6KB"
      },
      "outputs": [],
      "source": [
        "# Load the real images with encrypted content (all of them are preprocessed already)\n",
        "\n",
        "\n",
        "# Define the input shapes\n",
        "# embedding_shape = (embeddings_array.shape[1:])\n",
        "# embedding_shape = (1,128)\n",
        "# input_shape = (224, 224,3)  # Specify the dimensions of your images\n",
        "\n",
        "# print(embedding_shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHd3VjwRlQMO"
      },
      "outputs": [],
      "source": [
        "# # Define the GAN architecture\n",
        "# # Generator\n",
        "# def build_generator(input_shape, embedding_shape):\n",
        "#     input_img = Input(shape=input_shape)  # Input for the encrypted images\n",
        "#     input_embedding = Input(shape=embedding_shape)  # Input for the embeddings\n",
        "\n",
        "#     # Reshape embedding input to match image input shape\n",
        "#     embedding_reshaped = Reshape((1, 1, 128))(input_embedding)\n",
        "\n",
        "#     # Pad the embedding to match the spatial dimensions of the image input\n",
        "#     padding_height = input_img.shape[1] - embedding_reshaped.shape[1]\n",
        "#     padding_width = input_img.shape[2] - embedding_reshaped.shape[2]\n",
        "#     padding = ZeroPadding2D(padding=((0, padding_height), (0, padding_width)))(embedding_reshaped)\n",
        "\n",
        "\n",
        "\n",
        "#  # Concatenate image and embedding\n",
        "#     concatenated = Concatenate(axis=-1)([input_img, padding])\n",
        "\n",
        "#     # Generator network architecture\n",
        "#     x = Conv2D(64, kernel_size=3, strides=2, padding='same', activation='relu')(concatenated)\n",
        "#     x = Conv2D(128, kernel_size=3, strides=2, padding='same', activation='relu')(x)\n",
        "#     x = Conv2DTranspose(128, kernel_size=3, strides=2, padding='same', activation='relu')(x)\n",
        "#     x = Conv2DTranspose(64, kernel_size=3, strides=2, padding='same', activation='relu')(x)\n",
        "#     generated_img = Conv2DTranspose(3, kernel_size=3, strides=1, padding='same', activation='sigmoid')(x)  # Output image\n",
        "\n",
        "#     # Model\n",
        "#     generator = Model(inputs=[input_img, input_embedding], outputs=generated_img)\n",
        "#     return generator\n",
        "\n",
        "# # Discriminator\n",
        "# def build_discriminator(input_shape):\n",
        "#     input_img = Input(shape=input_shape)\n",
        "\n",
        "#     # Discriminator network architecture\n",
        "#     x = Conv2D(64, kernel_size=3, strides=2, padding='same', activation=LeakyReLU(alpha=0.2))(input_img)\n",
        "#     x = Conv2D(128, kernel_size=3, strides=2, padding='same', activation=LeakyReLU(alpha=0.2))(x)\n",
        "#     x = Flatten()(x)\n",
        "#     validity = Dense(1, activation='sigmoid')(x)  # Output validity\n",
        "\n",
        "#     # Model\n",
        "#     discriminator = Model(inputs=input_img, outputs=validity)\n",
        "#     return discriminator\n",
        "\n",
        "# # Adversarial Model\n",
        "# def build_adversarial_model(generator, discriminator, input_shape, embedding_shape):\n",
        "#     input_img = Input(shape=input_shape)\n",
        "#     input_embedding = Input(shape=embedding_shape)\n",
        "\n",
        "#     # Generate fake images\n",
        "#     generated_img = generator([input_img, input_embedding])\n",
        "\n",
        "#     # Disable discriminator training during generator training\n",
        "#     discriminator.trainable = False\n",
        "\n",
        "#     # Validity of generated images\n",
        "#     validity = discriminator(generated_img)\n",
        "\n",
        "#     # Adversarial model\n",
        "#     adversarial_model = Model(inputs=[input_img, input_embedding], outputs=validity)\n",
        "#     return adversarial_model\n",
        "\n",
        "# embedding_shape = (1,128)\n",
        "# input_shape = (224, 224,3)\n",
        "\n",
        "# # Build and compile discriminator\n",
        "# discriminator = build_discriminator(input_shape)\n",
        "# discriminator.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
        "\n",
        "# # Build generator\n",
        "# generator = build_generator(input_shape, embedding_shape)\n",
        "\n",
        "# adversarial_model = build_adversarial_model(generator,discriminator,input_shape,embedding_shape)\n",
        "\n",
        "# from tensorflow import keras\n",
        "# adversarial_model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "# discriminator.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "# generator.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 759
        },
        "id": "udrUXImfJES_",
        "outputId": "9fcb2ba8-a307-4b25-9cdd-92b6f55285a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2341, in predict_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2327, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2315, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2283, in predict_step\n        return self(x, training=False)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 1 of layer \"model_7\" is incompatible with the layer: expected shape=(None, 1, 128), found shape=(None, 128)\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-00cc7ef0b677>\u001b[0m in \u001b[0;36m<cell line: 45>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;31m# Train the discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0md_loss_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Real images should be labeled as 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mfake_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_embeddings\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Generate fake images using real embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0md_loss_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Fake images should be labeled as 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0md_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_loss_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_loss_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__predict_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2341, in predict_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2327, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2315, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2283, in predict_step\n        return self(x, training=False)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 1 of layer \"model_7\" is incompatible with the layer: expected shape=(None, 1, 128), found shape=(None, 128)\n"
          ]
        }
      ],
      "source": [
        "# import os\n",
        "# import cv2\n",
        "# import numpy as np\n",
        "# from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# # Function to load and preprocess images\n",
        "# def load_and_preprocess_image(image_path):\n",
        "#     img = cv2.imread(image_path)\n",
        "#     img = cv2.resize(img, (224, 224))  # Resize image\n",
        "#     img = img.astype('float32') / 255.0  # Normalize pixel values to [0, 1]\n",
        "#     return img\n",
        "\n",
        "# def calculate_precision_recall(conf_matrix):\n",
        "#     # Extract true positives, false positives, and false negatives from the confusion matrix\n",
        "#     TP = conf_matrix[1, 1]\n",
        "#     FP = conf_matrix[0, 1]\n",
        "#     FN = conf_matrix[1, 0]\n",
        "\n",
        "#     # Calculate precision\n",
        "#     precision = TP / (TP + FP)\n",
        "\n",
        "#     # Calculate recall\n",
        "#     recall = TP / (TP + FN)\n",
        "\n",
        "#     return precision, recall\n",
        "\n",
        "# # Load embeddings of real images without hidden content\n",
        "# embeddings_array = np.load('embeddings_array.npy')  # Assuming you have saved the embeddings in a numpy array\n",
        "# # print(embeddings_array)\n",
        "# # Define batch size\n",
        "# batch_size = 100\n",
        "# # print(len(embeddings_array))\n",
        "# # Define number of training steps\n",
        "# num_steps = len(embeddings_array) // batch_size\n",
        "# # print(num_steps)\n",
        "\n",
        "# # Define number of epochs\n",
        "# epochs = 60  # Adjusted number of epochs\n",
        "\n",
        "# # Initialize variables to accumulate predictions and true labels\n",
        "# all_predictions = []\n",
        "# all_true_labels = []\n",
        "\n",
        "# # Training loop\n",
        "# for epoch in range(epochs):\n",
        "#     print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "\n",
        "#     # Training loop\n",
        "#     for step in range(num_steps):\n",
        "#         # Prepare batch of real images without hidden content and their corresponding embeddings\n",
        "#         batch_embeddings = embeddings_array[step * batch_size: (step + 1) * batch_size]\n",
        "#         batch_images = []\n",
        "#         # print(\"hello world\")\n",
        "#         for img_name in os.listdir(input_images)[step * batch_size: (step + 1) * batch_size]:\n",
        "#             img_path = os.path.join(input_images, img_name)\n",
        "#             img = load_and_preprocess_image(img_path)\n",
        "#             batch_images.append(img)\n",
        "#         batch_images = np.array(batch_images)\n",
        "\n",
        "#         # # Generate fake embeddings (random noise for demonstration)\n",
        "#         # fake_embeddings = np.random.normal(0, 1, size=(batch_size, 128))  # Random noise\n",
        "\n",
        "\n",
        "#         #  # Train the discriminator\n",
        "#         # d_loss_real = discriminator.train_on_batch(batch_images, np.ones((batch_size, 1)))  # Real images should be labeled as 1\n",
        "#         # fake_images = generator.predict([batch_images, fake_embeddings])\n",
        "#         # d_loss_fake = discriminator.train_on_batch(fake_images, np.zeros((batch_size, 1)))  # Fake images should be labeled as 0\n",
        "#         # d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "#         # Train the discriminator\n",
        "#         d_loss_real = discriminator.train_on_batch(batch_images, np.ones((batch_size, 1)))  # Real images should be labeled as 1\n",
        "#         fake_images = generator.predict([batch_images, batch_embeddings])  # Generate fake images using real embeddings\n",
        "#         d_loss_fake = discriminator.train_on_batch(fake_images, np.zeros((batch_size, 1)))  # Fake images should be labeled as 0\n",
        "#         d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "#       # Train the generator (adversarial model)\n",
        "#         g_loss = adversarial_model.train_on_batch([batch_images, batch_embeddings], np.ones((batch_size, 1)))  # Fool the discriminator\n",
        "\n",
        "\n",
        "#         # Calculate discriminator accuracy\n",
        "#         discriminator_accuracy_real = d_loss_real[1]\n",
        "#         discriminator_accuracy_fake = d_loss_fake[1]\n",
        "#         discriminator_accuracy = 0.5 * (discriminator_accuracy_real + discriminator_accuracy_fake)\n",
        "\n",
        "#         # Calculate generator accuracy\n",
        "#         generator_accuracy = g_loss[1]\n",
        "\n",
        "#         # Print progress\n",
        "#         print(f\"Step: {step}/{num_steps}, D_loss: {d_loss}, G_loss: {g_loss}, D_accuracy: {discriminator_accuracy}, G_accuracy: {generator_accuracy}\")\n",
        "\n",
        "#         # Get discriminator predictions for real and fake images\n",
        "#         discriminator_predictions_real = discriminator.predict(batch_images)\n",
        "#         discriminator_predictions_fake = discriminator.predict(fake_images)\n",
        "\n",
        "#         # Threshold discriminator predictions\n",
        "#         discriminator_predictions_real = (discriminator_predictions_real >= 0.5).astype(int)\n",
        "#         discriminator_predictions_fake = (discriminator_predictions_fake < 0.5).astype(int)\n",
        "\n",
        "#         # Accumulate predictions and true labels\n",
        "#         all_predictions.extend(discriminator_predictions_real)\n",
        "#         all_predictions.extend(discriminator_predictions_fake)\n",
        "#         all_true_labels.extend([1] * batch_size)\n",
        "#         all_true_labels.extend([0] * batch_size)\n",
        "\n",
        "\n",
        "#     conf_matrix = confusion_matrix(all_true_labels, all_predictions)\n",
        "\n",
        "#         # Print confusion matrix\n",
        "#     print(\"Confusion Matrix:\")\n",
        "#     print(conf_matrix)\n",
        "\n",
        "#         # Calculate precision and recall\n",
        "#     precision, recall = calculate_precision_recall(conf_matrix)\n",
        "\n",
        "#         # Print precision and recall\n",
        "#     print(f\"Precision: {precision}\")\n",
        "#     print(f\"Recall: {recall}\")\n",
        "\n",
        "# # Save the trained models if needed\n",
        "# discriminator.save('discriminator.h5')\n",
        "# generator.save('generator.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8M31IJs3Ozq6"
      },
      "outputs": [],
      "source": [
        "generator.save('generator_saved_model_tf', save_format='tf')\n",
        "!zip -r generator_saved_model_tf.zip generator_saved_model_tf\n",
        "from google.colab import files\n",
        "\n",
        "files.download('generator_saved_model_tf.zip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wOijBZBH4jGv"
      },
      "outputs": [],
      "source": [
        "tf.saved_model.save(generator, 'generator_saved_model_tf2')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFa6mB7qPRx1"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "# Zip the SavedModel directory\n",
        "shutil.make_archive('generator_saved_model_tf2', 'zip', 'generator_saved_model_tf2')\n",
        "\n",
        "# Download the zip file\n",
        "from google.colab import files\n",
        "files.download('generator_saved_model_tf2.zip')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "zBPVPSGePU4R",
        "outputId": "6dbcc4f4-7486-42e8-cffb-ce6f703865ed"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_006a0b10-4f8b-414d-9b2c-aa10925be084\", \"generator_saved_model_tf.zip\", 1398528)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "kbVd7iCNbhvO",
        "outputId": "80864c89-8d0c-4f01-8ae0-d68480886ed0"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_809bfce6-6c6c-443b-9be2-47e70f97a97e\", \"generator.h5\", 1522216)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Specify the path to your H5 file in Colab\n",
        "h5_file_path = 'generator.h5'  # Update this with your actual file path\n",
        "\n",
        "# Download the H5 file to your local system\n",
        "files.download(h5_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0e68ijOVuyw2"
      },
      "outputs": [],
      "source": [
        "# Assuming you have loaded the trained generator model\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "generator = load_model('generator.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcfW1XrMwHEl",
        "outputId": "563e5a48-6216-4972-d286-c3f4c1a5cece"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "# import gensim.downloader as api\n",
        "# from nltk.tokenize import word_tokenize\n",
        "# import numpy as np\n",
        "\n",
        "# !pip install nltk\n",
        "\n",
        "# import nltk\n",
        "# nltk.download('punkt')\n",
        "\n",
        "\n",
        "# # Load pre-trained Word2Vec model\n",
        "# w2v_model = api.load('word2vec-google-news-300')\n",
        "\n",
        "# # Function to convert text to embedding\n",
        "# def text_to_embedding(text, embedding_size=128):\n",
        "#     # Tokenize the text\n",
        "#     tokens = word_tokenize(text)\n",
        "\n",
        "#     # Initialize an empty list to store word vectors\n",
        "#     word_vectors = []\n",
        "\n",
        "#     # Iterate over tokens and get their word vectors\n",
        "#     for token in tokens:\n",
        "#         try:\n",
        "#             # Get the word vector for the token\n",
        "#             word_vector = w2v_model[token]\n",
        "#             word_vectors.append(word_vector)\n",
        "#         except KeyError:\n",
        "#             # Ignore tokens not found in the vocabulary\n",
        "#             pass\n",
        "\n",
        "#     # Average the word vectors to get the text embedding\n",
        "#     if word_vectors:\n",
        "#         text_embedding = np.mean(word_vectors, axis=0)\n",
        "#     else:\n",
        "#         # If no word vectors found, use zeros\n",
        "#         text_embedding = np.zeros((embedding_size,))\n",
        "\n",
        "#     # Pad or truncate the embedding to have the desired size\n",
        "#     if len(text_embedding) < embedding_size:\n",
        "#         text_embedding = np.pad(text_embedding, ((0, embedding_size - len(text_embedding))), mode='constant')\n",
        "#     elif len(text_embedding) > embedding_size:\n",
        "#         text_embedding = text_embedding[:embedding_size]\n",
        "\n",
        "#     # Reshape to match the desired shape (1, 128)\n",
        "#     text_embedding = text_embedding.reshape((1, embedding_size))\n",
        "\n",
        "#     return text_embedding\n",
        "\n",
        "# # Example usage\n",
        "# # text = \"Your text here\"\n",
        "# # text_embedding = text_to_embedding(text)\n",
        "# # print(text_embedding.shape)  # Output: (1, 128)\n",
        "\n",
        "\n",
        "# # Words to generate embeddings for\n",
        "# words = [\"hello\", \"world\", \"suresh\", \"ramesh\", \"power\", \"affirmations\", \"30days\", \"meditation\", \"universe\", \"faith\"]\n",
        "\n",
        "# # Generate embeddings for each word\n",
        "# embeddings = []\n",
        "# for word in words:\n",
        "#     embedding = text_to_embedding(word)\n",
        "#     embeddings.append(embedding)\n",
        "# print(embeddings[0].shape[0])\n",
        "\n",
        "# # Convert embeddings to array format\n",
        "# text_embeddings = np.concatenate(embeddings, axis=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDisCWU7r0zZ",
        "outputId": "4e44c767-581b-460c-e01b-207b08cde8be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 370ms/step\n",
            "1/1 [==============================] - 0s 289ms/step\n",
            "1/1 [==============================] - 0s 261ms/step\n",
            "1/1 [==============================] - 0s 287ms/step\n",
            "1/1 [==============================] - 0s 264ms/step\n",
            "1/1 [==============================] - 0s 268ms/step\n",
            "1/1 [==============================] - 0s 263ms/step\n",
            "1/1 [==============================] - 1s 551ms/step\n",
            "1/1 [==============================] - 1s 783ms/step\n",
            "1/1 [==============================] - 1s 754ms/step\n",
            "Embedded images saved successfully.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "\n",
        "# Assuming you have already defined the load_and_preprocess_image function\n",
        "\n",
        "# Define the directory containing the images\n",
        "images_directory = '/content/drive/My Drive/Dataset/S_Steg/'\n",
        "\n",
        "# Load all image paths from the directory\n",
        "image_paths = [os.path.join(images_directory, filename) for filename in os.listdir(images_directory)]\n",
        "\n",
        "# Now, iterate over each image and its corresponding text embedding\n",
        "for i, image_path in enumerate(image_paths):\n",
        "    # Load and preprocess the image\n",
        "    input_img = load_and_preprocess_image(image_path)\n",
        "\n",
        "    # Expand dimensions to create batches\n",
        "    input_img_batch = np.expand_dims(input_img, axis=0)\n",
        "    input_embedding_batch = np.expand_dims(embeddings[i], axis=0)\n",
        "\n",
        "    # Predict the embedded image using the generator\n",
        "    embedded_image = generator.predict([input_img_batch, input_embedding_batch])\n",
        "\n",
        "    # Save the embedded image\n",
        "    output_path = '/content/drive/My Drive/Embedded_Images/image_{}.jpg'.format(i)\n",
        "    cv2.imwrite(output_path, embedded_image[0])\n",
        "\n",
        "print(\"Embedded images saved successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWPobk-y-yro"
      },
      "outputs": [],
      "source": [
        "# Assuming your new image path is stored in img_path variable\n",
        "img_path = '/content/drive/My Drive/Dataset/S_Steg/00085.jpg'\n",
        "input_img = load_and_preprocess_image(img_path)\n",
        "# print(img.shape[0:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eaeDSO5OSMwS"
      },
      "outputs": [],
      "source": [
        "input_img_batch = np.expand_dims(input_img, axis=0)\n",
        "input_embedding_batch = np.expand_dims(text_embedding, axis=0)\n",
        "\n",
        "# Now, predict the embedded image\n",
        "embedded_image = generator.predict([input_img_batch, input_embedding_batch])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQ4nS2DTTGQn"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "# Rescale the embedded image\n",
        "rescaled_embedded_image = embedded_image[0]  # Assuming embedded_image is a batch with a single image\n",
        "rescaled_embedded_image = cv2.resize(rescaled_embedded_image, (224, 224))  # Replace desired_width and desired_height with your desired dimensions\n",
        "\n",
        "# Save the rescaled embedded image to your drive\n",
        "output_path = '/content/drive/My Drive/Embedded_Images' # Update this path to your desired output directory\n",
        "output_filename = 'rescaled_embedded_image.jpg'  # Update this filename as needed\n",
        "\n",
        "cv2.imwrite(os.path.join(output_path, output_filename), rescaled_embedded_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrFYTDnUOfIg",
        "outputId": "e59c121d-b9ae-48db-e6d8-529f3e48a13e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 367ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 181ms/step\n"
          ]
        }
      ],
      "source": [
        "# # Assuming your new image path is stored in img_path variable\n",
        "# img_path = '/content/drive/My Drive/Inputs/Steg_img/inp_1.jpg'\n",
        "\n",
        "# # Load the new image\n",
        "# new_img = load_img(img_path, target_size=(224, 224))\n",
        "# new_img_array = img_to_array(new_img)\n",
        "\n",
        "# # Preprocess the new image and extract features\n",
        "# new_img_features = extract_features(new_img_array)\n",
        "\n",
        "# # Convert the extracted features into a numpy array\n",
        "# new_img_features_array = np.array([list(new_img_features.values())])\n",
        "\n",
        "# # Use the trained contrastive model to generate embeddings for the new image\n",
        "# input_embedding = c_model.predict([new_img_features_array, new_img_features_array])\n",
        "\n",
        "# # Assuming input_img is a single image and input_embedding is its corresponding embedding\n",
        "# # Add an extra dimension to each to represent the batch dimension\n",
        "# input_img_batch = np.expand_dims(input_img, axis=0)\n",
        "# input_embedding_batch = np.expand_dims(input_embedding, axis=0)\n",
        "\n",
        "# # Now, predict the embedded image\n",
        "# embedded_image = generator.predict([input_img_batch, input_embedding_batch])\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIE7awQwAaJU",
        "outputId": "161b46b5-a0ce-4699-8a92-ba63298acd4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 92ms/step\n"
          ]
        }
      ],
      "source": [
        "# embedded_image = generator.predict([input_img_batch,input_embedding_batch])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtCDzgUcP0pw",
        "outputId": "7ba65a55-48a8-48d2-d6e9-4604c721e180"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# import cv2\n",
        "\n",
        "# # Rescale the embedded image\n",
        "# rescaled_embedded_image = embedded_image[0]  # Assuming embedded_image is a batch with a single image\n",
        "# rescaled_embedded_image = cv2.resize(rescaled_embedded_image, (224, 224))  # Replace desired_width and desired_height with your desired dimensions\n",
        "\n",
        "# # Save the rescaled embedded image to your drive\n",
        "# output_path = '/content/drive/My Drive/Embedded_Images' # Update this path to your desired output directory\n",
        "# output_filename = 'rescaled_embedded_image.jpg'  # Update this filename as needed\n",
        "\n",
        "# cv2.imwrite(os.path.join(output_path, output_filename), rescaled_embedded_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IbjhLbEXVGAF",
        "outputId": "a52920c9-3fc4-42a0-a41d-343d01240cba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.13.0\n",
            "  Downloading tensorflow-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (524.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m524.1/524.1 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (24.3.7)\n",
            "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.13.0)\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.62.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (3.9.0)\n",
            "Collecting keras<2.14,>=2.13.1 (from tensorflow==2.13.0)\n",
            "  Downloading keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (16.0.6)\n",
            "Collecting numpy<=1.24.3,>=1.22 (from tensorflow==2.13.0)\n",
            "  Downloading numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.16.0)\n",
            "Collecting tensorboard<2.14,>=2.13 (from tensorflow==2.13.0)\n",
            "  Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.14,>=2.13.0 (from tensorflow==2.13.0)\n",
            "  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.8/440.8 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (2.4.0)\n",
            "Collecting typing-extensions<4.6.0,>=3.6.6 (from tensorflow==2.13.0)\n",
            "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0) (0.36.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.13.0) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2.27.0)\n",
            "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.14,>=2.13->tensorflow==2.13.0)\n",
            "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (1.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.2.2)\n",
            "Installing collected packages: typing-extensions, tensorflow-estimator, numpy, keras, gast, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.10.0\n",
            "    Uninstalling typing_extensions-4.10.0:\n",
            "      Successfully uninstalled typing_extensions-4.10.0\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.15.0\n",
            "    Uninstalling tensorflow-estimator-2.15.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.15.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.2\n",
            "    Uninstalling numpy-1.25.2:\n",
            "      Successfully uninstalled numpy-1.25.2\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.15.0\n",
            "    Uninstalling keras-2.15.0:\n",
            "      Successfully uninstalled keras-2.15.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.4\n",
            "    Uninstalling gast-0.5.4:\n",
            "      Successfully uninstalled gast-0.5.4\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.0\n",
            "    Uninstalling google-auth-oauthlib-1.2.0:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.15.2\n",
            "    Uninstalling tensorboard-2.15.2:\n",
            "      Successfully uninstalled tensorboard-2.15.2\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.15.0\n",
            "    Uninstalling tensorflow-2.15.0:\n",
            "      Successfully uninstalled tensorflow-2.15.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.2.1+cu121 requires nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cudnn-cu12==8.9.2.26; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-nccl-cu12==2.19.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "sqlalchemy 2.0.28 requires typing-extensions>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "pydantic 2.6.4 requires typing-extensions>=4.6.1, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "pydantic-core 2.16.3 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "tf-keras 2.15.1 requires tensorflow<2.16,>=2.15, but you have tensorflow 2.13.0 which is incompatible.\n",
            "torch 2.2.1+cu121 requires typing-extensions>=4.8.0, but you have typing-extensions 4.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gast-0.4.0 google-auth-oauthlib-1.0.0 keras-2.13.1 numpy-1.24.3 tensorboard-2.13.0 tensorflow-2.13.0 tensorflow-estimator-2.13.0 typing-extensions-4.5.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "ede245b347e44604a94c4e7b5e14bcdb",
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# !pip install --upgrade tensorflow==2.13.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vaZAUTjSVm4J",
        "outputId": "aefaff56-95b0-4a28-8db8-a859bedd358b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 222, 222, 32)      896       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 111, 111, 32)      0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 109, 109, 64)      18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 54, 54, 64)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 52, 52, 128)       73856     \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 346112)            0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               177209856 \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 128)               65664     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 177368768 (676.61 MB)\n",
            "Trainable params: 177368768 (676.61 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# import numpy as np\n",
        "# import tensorflow as tf\n",
        "# from tensorflow.keras import layers, models\n",
        "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# # Define your CNN architecture\n",
        "# def create_cnn_model(input_shape, embedding_dim):\n",
        "#     model = models.Sequential([\n",
        "#         layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
        "#         layers.MaxPooling2D((2, 2)),\n",
        "#         layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "#         layers.MaxPooling2D((2, 2)),\n",
        "#         layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "#         layers.Flatten(),\n",
        "#         layers.Dense(512, activation='relu'),\n",
        "#         layers.Dense(embedding_dim)  # Output layer with size equal to embedding dimension\n",
        "#     ])\n",
        "#     return model\n",
        "\n",
        "# # Assuming 224x224 RGB images and 128-dimensional embeddings\n",
        "# input_shape = (224, 224, 3)\n",
        "# embedding_dim = 128  # Change this based on your actual embedding size\n",
        "# model = create_cnn_model(input_shape, embedding_dim)\n",
        "\n",
        "# model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "# model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FM3L1xC1za_f",
        "outputId": "e1dff7c8-e034-43ad-fd1f-2baad46ba18e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94765736/94765736 [==============================] - 1s 0us/step\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def build_model(input_shape, embedding_dim):\n",
        "    # Load a pre-trained ResNet50 model without the top classification layer\n",
        "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    base_model.trainable = False  # Freeze the base model\n",
        "\n",
        "    # Add custom layers on top for embedding prediction\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)  # Convert features to vectors\n",
        "    x = Dense(1024, activation='relu')(x)\n",
        "    embedding_prediction = Dense(embedding_dim, activation='linear')(x)  # Final layer matches embedding dimensions\n",
        "\n",
        "    model = Model(inputs=base_model.input, outputs=embedding_prediction)\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "    return model\n",
        "\n",
        "# Assuming 224x224 RGB images and 128-dimensional text embeddings\n",
        "input_shape = (224, 224, 3)\n",
        "embedding_dim = 128  # Adjust based on your text embedding dimension\n",
        "model = build_model(input_shape, embedding_dim)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQyHqYLSzjS3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "import os\n",
        "\n",
        "# Path to the directory where your images are stored\n",
        "image_directory = '/content/drive/My Drive/Embedded_Images'\n",
        "\n",
        "# Path to the NumPy file containing text embeddings\n",
        "# embeddings_file = '/content/drive/My Drive/path_to_your_embeddings.npy'\n",
        "\n",
        "# embeddings_array\n",
        "\n",
        "# Load text embeddings\n",
        "# text_embeddings = np.load(embeddings_file)\n",
        "\n",
        "# Assuming all your images are of the same size, adjust the target_size accordingly\n",
        "target_size = (224, 224)  # This should match the input shape expected by your model\n",
        "\n",
        "# Load and preprocess images\n",
        "def load_images(image_directory, target_size):\n",
        "    images = []\n",
        "    image_files = sorted(os.listdir(image_directory))  # Ensure consistent ordering\n",
        "    for file_name in image_files:\n",
        "        if file_name.endswith('.jpg'):  # Adjust this based on your file types\n",
        "            img_path = os.path.join(image_directory, file_name)\n",
        "            img = load_img(img_path, target_size=target_size)\n",
        "            img_array = img_to_array(img) / 255.0  # Normalize images to [0, 1]\n",
        "            images.append(img_array)\n",
        "    return np.array(images)\n",
        "\n",
        "images = load_images(image_directory, target_size)\n",
        "\n",
        "# Make sure the number of images matches the number of text embeddings\n",
        "assert len(images) == len(text_embeddings), \"Mismatch in the number of images and text embeddings\"\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8ma5U7Xyrdr",
        "outputId": "b1d510e7-6b40-451d-f506-1dc1c6cc2fcb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.4209 - val_loss: 0.0878\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0798 - val_loss: 0.1081\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0913 - val_loss: 0.1427\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.1236 - val_loss: 0.1240\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.1092 - val_loss: 0.0897\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0796 - val_loss: 0.0639\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0565 - val_loss: 0.0497\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0437 - val_loss: 0.0444\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0383 - val_loss: 0.0434\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0372 - val_loss: 0.0430\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7c8b409ea470>"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(images, text_embeddings, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, batch_size=32, epochs=10, validation_data=(X_val, y_val))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FsqbubEOzuE6",
        "outputId": "c6ec9917-dd76-4768-ecbf-a48a566becea"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model.save('model.h5')\n",
        "\n",
        "# model_path = '/content/drive/My Drive/path_to_your_saved_model/model.h5'  # Adjust path as needed\n",
        "model = load_model(\"model.h5\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KLcnm9f0GP6"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "def preprocess_image(image_path, target_size):\n",
        "    img = load_img(image_path, target_size=target_size)\n",
        "    img_array = img_to_array(img) / 255.0  # Normalize to [0, 1]\n",
        "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
        "    return img_array\n",
        "\n",
        "# Example usage\n",
        "new_image_path = '/content/drive/My Drive/Embedded_Images/image_3.jpg'\n",
        "preprocessed_image = preprocess_image(new_image_path, target_size=(224, 224))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5_0xp660k5w",
        "outputId": "cb7989e1-9887-44dc-884b-11216eb67d0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 479ms/step\n"
          ]
        }
      ],
      "source": [
        "predicted_embedding = model.predict(preprocessed_image)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXI9sUUz1K9-",
        "outputId": "865a30ff-a6a9-422b-894a-2254280720c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decoded text: faith\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "# Assuming `original_embeddings` is a NumPy array of your training embeddings\n",
        "# and `original_texts` is a list of the corresponding original texts\n",
        "neighbors = NearestNeighbors(n_neighbors=1, metric='cosine')\n",
        "neighbors.fit(text_embeddings)\n",
        "\n",
        "# Find the nearest neighbor in the original embedding space\n",
        "distance, index = neighbors.kneighbors(predicted_embedding)\n",
        "\n",
        "# Retrieve the corresponding text\n",
        "decoded_text = words[index[0][0]]\n",
        "print(\"Decoded text:\", decoded_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "UIRgdHd80yxl",
        "outputId": "01d2431a-9ed5-417f-a089-989d01e7452a"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'path_to_word2vec.bin'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-5a7c26d9dc39>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Load pre-trained Word2Vec model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mw2v_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'path_to_word2vec.bin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Function to convert embedding to text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[1;32m   1717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m         \"\"\"\n\u001b[0;32m-> 1719\u001b[0;31m         return _load_word2vec_format(\n\u001b[0m\u001b[1;32m   1720\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1721\u001b[0m             \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_header\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mno_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[1;32m   2046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2047\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading projection weights from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2048\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2049\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mno_header\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2050\u001b[0m             \u001b[0;31m# deduce both vocab_size & vector_size from 1st pass over file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, compression, transport_params)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mtransport_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m     fobj = _shortcut_open(\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0mopen_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'errors'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'path_to_word2vec.bin'"
          ]
        }
      ],
      "source": [
        "from gensim.models import KeyedVectors\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Load pre-trained Word2Vec model\n",
        "w2v_model = KeyedVectors.load_word2vec_format('path_to_word2vec.bin', binary=True)\n",
        "\n",
        "# Function to convert embedding to text\n",
        "def embedding_to_text(embedding, top_n=5):\n",
        "    # Calculate similarity between the given embedding and all word vectors\n",
        "    similarities = w2v_model.similar_by_vector(embedding.flatten(), topn=top_n)\n",
        "\n",
        "    # Extract words from the similar words list\n",
        "    words = [word for word, _ in similarities]\n",
        "\n",
        "    # Combine words to form text\n",
        "    reconstructed_text = ' '.join(words)\n",
        "\n",
        "    return reconstructed_text\n",
        "\n",
        "# Example usage\n",
        "embedding = np.random.rand(300)  # Example embedding (300-dimensional)\n",
        "reconstructed_text = embedding_to_text(predicted_embedding)\n",
        "print(reconstructed_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cVjjQhSR5e0"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}