{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVuXKkm9GmrR"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import string\n",
        "\n",
        "import random\n",
        "import gensim.downloader as api\n",
        "\n",
        "# Load the Word2Vec model\n",
        "w2v_model = api.load('word2vec-google-news-300')\n",
        "\n",
        "# Access the model's vocabulary\n",
        "# For gensim versions 4.0.0 and above, use `key_to_index`\n",
        "vocab_list = list(w2v_model.key_to_index.keys())\n",
        "\n",
        "# For older gensim versions, you might need to use `vocab` instead:\n",
        "# vocab_list = list(w2v_model.vocab.keys())\n",
        "\n",
        "# Generate a 5000 membered array with words from the model's vocabulary\n",
        "words = [random.choice(vocab_list) for _ in range(5)]\n",
        "print(words[0])\n",
        "\n",
        "# Display the first 10 entries to verify\n",
        "# print(words_from_vocab[:10])\n",
        "\n",
        "\n",
        "# print(words)\n",
        "\n",
        "# Display the first 10 entries to verify\n",
        "# random_texts[:10]\n",
        "\n",
        "import gensim.downloader as api\n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "\n",
        "!pip install nltk\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "# Load pre-trained Word2Vec model\n",
        "w2v_model = api.load('word2vec-google-news-300')\n",
        "\n",
        "# Function to convert text to embedding\n",
        "def text_to_embedding(text, embedding_size=128):\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Initialize an empty list to store word vectors\n",
        "    word_vectors = []\n",
        "\n",
        "    # Iterate over tokens and get their word vectors\n",
        "    for token in tokens:\n",
        "        try:\n",
        "            # Get the word vector for the token\n",
        "            word_vector = w2v_model[token]\n",
        "            word_vectors.append(word_vector)\n",
        "        except KeyError:\n",
        "            # Ignore tokens not found in the vocabulary\n",
        "            pass\n",
        "\n",
        "    # Average the word vectors to get the text embedding\n",
        "    if word_vectors:\n",
        "        text_embedding = np.mean(word_vectors, axis=0)\n",
        "    else:\n",
        "        # If no word vectors found, use zeros\n",
        "        text_embedding = np.zeros((embedding_size,))\n",
        "\n",
        "    # Pad or truncate the embedding to have the desired size\n",
        "    if len(text_embedding) < embedding_size:\n",
        "        text_embedding = np.pad(text_embedding, ((0, embedding_size - len(text_embedding))), mode='constant')\n",
        "    elif len(text_embedding) > embedding_size:\n",
        "        text_embedding = text_embedding[:embedding_size]\n",
        "\n",
        "    # Reshape to match the desired shape (1, 128)\n",
        "    text_embedding = text_embedding.reshape((1, embedding_size))\n",
        "\n",
        "    return text_embedding\n",
        "\n",
        "# Example usage\n",
        "text = \"Your text here\"\n",
        "text_embedding = text_to_embedding(text)\n",
        "print(text_embedding.shape)  # Output: (1, 128)\n",
        "\n",
        "\n",
        "# Words to generate embeddings for\n",
        "# words = [\"hello\", \"world\", \"suresh\", \"ramesh\", \"power\", \"affirmations\", \"30days\", \"meditation\", \"universe\", \"faith\"]\n",
        "\n",
        "# Generate embeddings for each word\n",
        "embeddings = []\n",
        "for word in words:\n",
        "    embedding = text_to_embedding(word)\n",
        "    embeddings.append(embedding)\n",
        "print(embeddings[0].shape)\n",
        "\n",
        "# # Convert embeddings to array format\n",
        "embeddings_array = np.array(embeddings)\n",
        "\n",
        "\n",
        "# print(embeddings)\n",
        "\n",
        "# # Convert the list of embeddings into a numpy array\n",
        "# embeddings_array = np.array(embeddings)\n",
        "\n",
        "# Save the array to a file\n",
        "np.save(\"embeddings_array.npy\", embeddings_array)\n",
        "\n",
        "\n",
        "# Convert embeddings to array format\n",
        "# text_embeddings = np.concatenate(embeddings, axis=0)\n",
        "\n",
        "\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "# Step 1: Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def jpg_to_png(input_folder, output_folder):\n",
        "    # Create output folder if it doesn't exist\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    # Loop through all files in the input folder\n",
        "    for filename in os.listdir(input_folder):\n",
        "        if filename.endswith(\".jpg\"):\n",
        "            # Open the image file\n",
        "            with Image.open(os.path.join(input_folder, filename)) as img:\n",
        "                # Construct the output file path with .png extension\n",
        "                output_file = os.path.splitext(filename)[0] + \".png\"\n",
        "                # Save the image as PNG format in the output folder\n",
        "                img.save(os.path.join(output_folder, output_file), \"PNG\")\n",
        "                print(f\"Converted {filename} to {output_file}\")\n",
        "\n",
        "# Change these paths to your input and output folders\n",
        "input_folder = '/content/drive/My Drive/Dataset/just/'\n",
        "output_folder = \"/content/drive/My Drive/Dataset/png1/\"\n",
        "\n",
        "jpg_to_png(input_folder, output_folder)\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "!pip install Pillow\n",
        "\n",
        "# Step 1: Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "def load_and_preprocess_image(image_path):\n",
        "    img = cv2.imread(image_path)\n",
        "    img = cv2.resize(img, (224, 224))  # Resize image\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
        "    img = img.astype('float32') / 255.0  # Normalize pixel values to [0, 1]\n",
        "    return img\n",
        "\n",
        "def encode_text_into_image(preprocessed_img, text, output_path):\n",
        "    encoded_img = Image.fromarray((preprocessed_img * 255).astype('uint8'))  # Convert back to PIL image\n",
        "    binary_text = ''.join(format(ord(char), '08b') for char in text)\n",
        "    binary_text += '1111111111111110'  # Delimiter\n",
        "\n",
        "    data_index = 0\n",
        "    for row in range(encoded_img.height):\n",
        "        for col in range(encoded_img.width):\n",
        "            if data_index < len(binary_text):\n",
        "                pixel = list(encoded_img.getpixel((col, row)))\n",
        "                for n in range(3):\n",
        "                    if data_index < len(binary_text):\n",
        "                        pixel[n] = pixel[n] & ~1 | int(binary_text[data_index])\n",
        "                        data_index += 1\n",
        "                    else:\n",
        "                        break\n",
        "                encoded_img.putpixel((col, row), tuple(pixel))\n",
        "\n",
        "    encoded_img.save(output_path, format='PNG')\n",
        "\n",
        "def decode_text_from_image(image_path):\n",
        "    img = Image.open(image_path)\n",
        "    binary_text = ''\n",
        "    for row in range(img.height):\n",
        "        for col in range(img.width):\n",
        "            pixel = img.getpixel((col, row))\n",
        "            for n in pixel[:3]:\n",
        "                binary_text += str(n & 1)\n",
        "\n",
        "    delimiter_index = binary_text.find('1111111111111110')\n",
        "    if delimiter_index != -1:\n",
        "        binary_text = binary_text[:delimiter_index]\n",
        "\n",
        "    decoded_text = ''.join([chr(int(binary_text[i:i+8], 2)) for i in range(0, len(binary_text), 8)])\n",
        "\n",
        "    return decoded_text\n",
        "\n",
        "# Directory paths and words\n",
        "image_directory = '/content/drive/My Drive/Dataset/png1/'\n",
        "output_directory = '/content/drive/My Drive/Dataset/embedded1/'\n",
        "# words = ['Word #1', 'Word #2', ..., 'Word #50']\n",
        "\n",
        "# Ensure output directory exists\n",
        "if not os.path.exists(output_directory):\n",
        "    os.makedirs(output_directory)\n",
        "\n",
        "# Process each image\n",
        "for i, image_name in enumerate(os.listdir(image_directory)):\n",
        "    if image_name.endswith('.png'):\n",
        "        image_path = os.path.join(image_directory, image_name)\n",
        "        preprocessed_img = load_and_preprocess_image(image_path)\n",
        "\n",
        "        output_path = os.path.join(output_directory, f\"encoded_{image_name}\")\n",
        "        encode_text_into_image(preprocessed_img, words[i], output_path)\n",
        "\n",
        "        # Decode for demonstration (optional)\n",
        "        # decoded_text = decode_text_from_image(output_path)\n",
        "        # print(f\"Decoded text from {output_path}: {decoded_text}\")\n",
        "\n",
        "\n",
        "from tensorflow.keras.layers import Input, Concatenate, Conv2D, Conv2DTranspose, Flatten, Dense, LeakyReLU, ZeroPadding2D, Reshape\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "# Define the GAN architecture\n",
        "# Generator\n",
        "def build_generator(input_shape, embedding_shape):\n",
        "    input_img = Input(shape=input_shape)  # Input for the original images\n",
        "    input_embedding = Input(shape=embedding_shape)  # Input for the embeddings\n",
        "\n",
        "    # Reshape embedding input to match the shape of the original image\n",
        "    embedding_reshaped = Reshape((1, 1, embedding_shape[-1]))(input_embedding)\n",
        "\n",
        "    # Pad the embedding to match the spatial dimensions of the image input\n",
        "    padding_height = input_shape[0] - 1\n",
        "    padding_width = input_shape[1] - 1\n",
        "    padding = ZeroPadding2D(padding=((0, padding_height), (0, padding_width)))(embedding_reshaped)\n",
        "\n",
        "    # Concatenate original image and embedding\n",
        "    concatenated = Concatenate(axis=-1)([input_img, padding])\n",
        "\n",
        "    # # Generator network architecture\n",
        "    # x = Conv2D(64, kernel_size=3, strides=2, padding='same', activation='relu')(concatenated)\n",
        "    # x = Conv2D(128, kernel_size=3, strides=2, padding='same', activation='relu')(x)\n",
        "    # x = Conv2DTranspose(128, kernel_size=3, strides=2, padding='same', activation='relu')(x)\n",
        "    # x = Conv2DTranspose(64, kernel_size=3, strides=2, padding='same', activation='relu')(x)\n",
        "    # generated_img = Conv2DTranspose(3, kernel_size=3, strides=1, padding='same', activation='sigmoid')(x)  # Output image\n",
        "\n",
        "\n",
        "    x = Conv2D(64, kernel_size=3, strides=2, padding='same')(concatenated)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "\n",
        "    x = Conv2D(128, kernel_size=3, strides=2, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "\n",
        "    x = Conv2DTranspose(128, kernel_size=3, strides=2, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "    x = Conv2DTranspose(64, kernel_size=3, strides=2, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "    generated_img = Conv2DTranspose(3, kernel_size=3, strides=1, padding='same', activation='sigmoid')(x)\n",
        "\n",
        "    # Model\n",
        "    generator = Model(inputs=[input_img, input_embedding], outputs=generated_img)\n",
        "    return generator\n",
        "\n",
        "# Discriminator\n",
        "def build_discriminator(input_shape, embedding_shape):\n",
        "    input_img = Input(shape=input_shape)  # Input for images with embedded text\n",
        "    input_embedding = Input(shape=embedding_shape)  # Input for embeddings\n",
        "\n",
        "        # Reshape embedding input to match the shape of the original image\n",
        "    embedding_reshaped = Reshape((1, 1, embedding_shape[-1]))(input_embedding)\n",
        "\n",
        "    # Pad the embedding to match the spatial dimensions of the image input\n",
        "    padding_height = input_shape[0] - 1\n",
        "    padding_width = input_shape[1] - 1\n",
        "    padding = ZeroPadding2D(padding=((0, padding_height), (0, padding_width)))(embedding_reshaped)\n",
        "\n",
        "    # Concatenate image and embedding\n",
        "    concatenated = Concatenate(axis=-1)([input_img, padding])\n",
        "\n",
        "    x = Conv2D(64, kernel_size=3, strides=2, padding='same')(concatenated)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "\n",
        "    x = Conv2D(128, kernel_size=3, strides=2, padding='same')(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "\n",
        "    # Discriminator network architecture\n",
        "    # x = Conv2D(64, kernel_size=3, strides=2, padding='same', activation=LeakyReLU(alpha=0.2))(concatenated)\n",
        "    # x = Conv2D(128, kernel_size=3, strides=2, padding='same', activation=LeakyReLU(alpha=0.2))(x)\n",
        "    x = Flatten()(x)\n",
        "    validity = Dense(1, activation='sigmoid')(x)  # Output validity\n",
        "\n",
        "    # Model\n",
        "    discriminator = Model(inputs=[input_img, input_embedding], outputs=validity)\n",
        "    return discriminator\n",
        "\n",
        "# Adversarial Model\n",
        "def build_adversarial_model(generator, discriminator, input_shape, embedding_shape):\n",
        "    input_img = Input(shape=input_shape)  # Input for original images\n",
        "    input_embedding = Input(shape=embedding_shape)  # Input for embeddings\n",
        "\n",
        "    # Generate fake images\n",
        "    generated_img = generator([input_img, input_embedding])\n",
        "\n",
        "    # Disable discriminator training during generator training\n",
        "    discriminator.trainable = False\n",
        "\n",
        "    # Validity of generated images\n",
        "    validity = discriminator([generated_img, input_embedding])\n",
        "\n",
        "    # Adversarial model\n",
        "    adversarial_model = Model(inputs=[input_img, input_embedding], outputs=validity)\n",
        "    return adversarial_model\n",
        "\n",
        "embedding_shape = (1, 128)  # Assuming text embeddings are of shape (1, 128)\n",
        "input_shape = (224, 224, 3)\n",
        "\n",
        "# Build and compile discriminator\n",
        "discriminator = build_discriminator(input_shape, embedding_shape)\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
        "\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "\n",
        "# Define learning rate schedule\n",
        "initial_learning_rate = 0.0001\n",
        "lr_schedule = ExponentialDecay(\n",
        "    initial_learning_rate=initial_learning_rate,\n",
        "    decay_steps=1000,\n",
        "    decay_rate=0.96,\n",
        "    staircase=True)\n",
        "\n",
        "# Optimizers with learning rate schedule\n",
        "optimizer_gen = tf.keras.optimizers.Adam(learning_rate=lr_schedule, beta_1=0.5)\n",
        "optimizer_disc = tf.keras.optimizers.Adam(learning_rate=lr_schedule, beta_1=0.5)\n",
        "\n",
        "# Build generator\n",
        "generator = build_generator(input_shape, embedding_shape)\n",
        "\n",
        "\n",
        "# Build adversarial model\n",
        "adversarial_model = build_adversarial_model(generator, discriminator, input_shape, embedding_shape)\n",
        "adversarial_model.compile(optimizer=optimizer_gen, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Compile discriminator (separately for fine-tuning)\n",
        "discriminator.compile(optimizer=optimizer_disc, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Compile generator (separately for fine-tuning)\n",
        "generator.compile(optimizer=optimizer_gen, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# Function to load and preprocess images\n",
        "def load_and_preprocess_image(image_path):\n",
        "    img = cv2.imread(image_path)\n",
        "    img = cv2.resize(img, (224, 224))  # Resize image\n",
        "    # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
        "    # img = img.astype('float32') / 255.0  # Normalize pixel values to [0, 1]\n",
        "    return img\n",
        "\n",
        "def calculate_precision_recall(conf_matrix):\n",
        "    # Extract true positives, false positives, and false negatives from the confusion matrix\n",
        "    TP = conf_matrix[1, 1]\n",
        "    FP = conf_matrix[0, 1]\n",
        "    FN = conf_matrix[1, 0]\n",
        "\n",
        "    # Calculate precision\n",
        "    precision = TP / (TP + FP)\n",
        "\n",
        "    # Calculate recall\n",
        "    recall = TP / (TP + FN)\n",
        "\n",
        "    return precision, recall\n",
        "\n",
        "# Load embeddings of real images without hidden content\n",
        "embeddings_array = np.load('embeddings_array.npy')\n",
        "print(embeddings_array[0].shape)\n",
        "# print(embeddings_array)\n",
        "# Define batch size\n",
        "batch_size = 1\n",
        "# print(len(embeddings_array))\n",
        "# Define number of training steps\n",
        "num_steps = len(embeddings_array) // batch_size\n",
        "# print(num_steps)\n",
        "\n",
        "# Define number of epochs\n",
        "epochs = 1  # Adjusted number of epochs\n",
        "\n",
        "# Initialize variables to accumulate predictions and true labels\n",
        "all_predictions = []\n",
        "all_true_labels = []\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "\n",
        "    # Training loop\n",
        "    for step in range(num_steps):\n",
        "        # Prepare batch of real images without hidden content and their corresponding embeddings\n",
        "        batch_embeddings = embeddings_array[step * batch_size: (step + 1) * batch_size]\n",
        "        # Adjust embeddings shape\n",
        "        # batch_embeddings = np.expand_dims(batch_embeddings, axis=1)  # Now has shape (1, 1, 128)\n",
        "        batch_images = []\n",
        "        for img_name in os.listdir(input_images)[step * batch_size: (step + 1) * batch_size]:\n",
        "            img_path = os.path.join(input_images, img_name)\n",
        "            img = load_and_preprocess_image(img_path)\n",
        "            batch_images.append(img)\n",
        "        batch_images = np.array(batch_images)\n",
        "\n",
        "        # Prepare batch of embedded images and their corresponding embeddings\n",
        "        batch_embedded_images = []\n",
        "        for img_name in os.listdir(embedded_images)[step * batch_size: (step + 1) * batch_size]:\n",
        "            img_path = os.path.join(embedded_images, img_name)\n",
        "            img = load_and_preprocess_image(img_path)\n",
        "            # img = cv2.imread(image_path)\n",
        "            batch_embedded_images.append(img)\n",
        "        print(batch_embedded_images[0].shape)\n",
        "        batch_embedded_images = np.array(batch_embedded_images)\n",
        "\n",
        "        # Train the discriminator\n",
        "        d_loss_real = discriminator.train_on_batch([batch_embedded_images, batch_embeddings], tf.ones((batch_size,1)))  # Real images should be labeled as 1\n",
        "        fake_images = generator.predict([batch_images, batch_embeddings])  # Generate fake images using real embeddings\n",
        "        d_loss_fake = discriminator.train_on_batch([fake_images, batch_embeddings], tf.zeros((batch_size,1)))  # Fake images should be labeled as 0\n",
        "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "        # Train the generator (adversarial model)\n",
        "        g_loss = adversarial_model.train_on_batch([batch_images, batch_embeddings], np.ones((batch_size, 1)))  # Fool the discriminator\n",
        "\n",
        "        # Calculate discriminator accuracy\n",
        "        discriminator_accuracy_real = d_loss_real[1]\n",
        "        discriminator_accuracy_fake = d_loss_fake[1]\n",
        "        discriminator_accuracy = 0.5 * (discriminator_accuracy_real + discriminator_accuracy_fake)\n",
        "\n",
        "        # Calculate generator accuracy\n",
        "        generator_accuracy = g_loss[1]\n",
        "\n",
        "        # Print progress\n",
        "        print(f\"Step: {step}/{num_steps}, D_loss: {d_loss}, G_loss: {g_loss}, D_accuracy: {discriminator_accuracy}, G_accuracy: {generator_accuracy}\")\n",
        "\n",
        "        # Get discriminator predictions for real and fake images\n",
        "        discriminator_predictions_real = discriminator.predict([batch_embedded_images, batch_embeddings])\n",
        "        discriminator_predictions_fake = discriminator.predict([fake_images, batch_embeddings])\n",
        "\n",
        "        # Threshold discriminator predictions\n",
        "        discriminator_predictions_real = (discriminator_predictions_real >= 0.5).astype(int)\n",
        "        discriminator_predictions_fake = (discriminator_predictions_fake < 0.5).astype(int)\n",
        "\n",
        "        # Accumulate predictions and true labels\n",
        "        all_predictions.extend(discriminator_predictions_real)\n",
        "        all_predictions.extend(discriminator_predictions_fake)\n",
        "        all_true_labels.extend([1] * batch_size)\n",
        "        all_true_labels.extend([0] * batch_size)\n",
        "\n",
        "    conf_matrix = confusion_matrix(all_true_labels, all_predictions)\n",
        "\n",
        "    # Print confusion matrix\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(conf_matrix)\n",
        "\n",
        "    # Calculate precision and recall\n",
        "    precision, recall = calculate_precision_recall(conf_matrix)\n",
        "\n",
        "    # Print precision and recall\n",
        "    print(f\"Precision: {precision}\")\n",
        "    print(f\"Recall: {recall}\")\n",
        "\n",
        "# Save the trained models if needed\n",
        "discriminator.save('discriminator.h5')\n",
        "generator.save('generator.h5')\n",
        "\n",
        "\n",
        "#Decoder architecture\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout,Reshape\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, TimeDistributed, Lambda, RepeatVector\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "# Load VGG16 as the base model, excluding the top (classification) layer\n",
        "base_model = VGG16(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
        "\n",
        "# Set the layers of the base model to be non-trainable to preserve pre-trained features\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Add new layers on top of the base model\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)  # Reduces the features to a 1D vector to prevent overfitting\n",
        "\n",
        "# Optional: Add a Dropout layer for regularization\n",
        "x = Dropout(0.5)(x)\n",
        "\n",
        "# Add a Dense layer to allow the network to learn complex patterns\n",
        "x = Dense(512, activation='relu')(x)\n",
        "\n",
        "# Final Dense layer with output shape (128,) to match text embeddings\n",
        "final_output = Dense(128, activation='relu')(x)\n",
        "\n",
        "\n",
        "# Reshape the output to (None, 1, 128)\n",
        "final_output_reshaped = Reshape((1, 128))(final_output)\n",
        "\n",
        "# Define the new model, from base model input to the new output\n",
        "encoder_model = Model(inputs=base_model.input, outputs=final_output_reshaped)\n",
        "\n",
        "# Display the model summary to verify the architecture\n",
        "# encoder_model.summary()\n",
        "\n",
        "\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, RepeatVector,TimeDistributed\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "vocab_size = 4500  # Adjust based on your dataset\n",
        "max_seq_length = 20  # Maximum length of text sequences\n",
        "embedding_dim = 128  # Dimension of the embeddings\n",
        "\n",
        "# Decoder Model\n",
        "decoder_inputs = Input(shape=(1, embedding_dim))  # Adjusted to match embedding shape\n",
        "\n",
        "# Repeat the encoder's output to match the desired sequence length\n",
        "# decoder_repeated = RepeatVector(max_seq_length)(decoder_inputs)\n",
        "\n",
        "# LSTM layer\n",
        "# decoder_lstm = LSTM(256, return_sequences=True)(decoder_inputs)\n",
        "\n",
        "# Repeat the input for the sequence length to match the LSTM input requirements\n",
        "repeated_decoder_inputs = RepeatVector(max_seq_length)(Lambda(lambda x: K.squeeze(x, axis=1))(decoder_inputs))\n",
        "\n",
        "# LSTM layer\n",
        "decoder_lstm = LSTM(256, return_sequences=True, name=\"decoder_lstm\")(repeated_decoder_inputs)\n",
        "\n",
        "# Use TimeDistributed to apply Dense layer to each time step independently\n",
        "decoder_time_dist = TimeDistributed(Dense(vocab_size, activation='softmax'))(decoder_lstm)\n",
        "\n",
        "# Building the model with TimeDistributed Dense layer\n",
        "decoder_model = Model(inputs=decoder_inputs, outputs=decoder_time_dist)\n",
        "\n",
        "# # Predicting each word in the sequence\n",
        "# decoder_dense = Dense(vocab_size, activation='softmax')\n",
        "# decoder_outputs = decoder_dense(decoder_lstm)\n",
        "\n",
        "# # Building the model\n",
        "# decoder_model = Model(inputs=decoder_inputs, outputs=decoder_outputs)\n",
        "\n",
        "# Model compilation\n",
        "decoder_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Display the model architecture\n",
        "# decoder_model.summary()\n",
        "\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "\n",
        "\n",
        "# Define the full model that chains encoder and decoder\n",
        "def build_end_to_end_model(encoder, decoder):\n",
        "    # Define the input for the full model, which is the input for the encoder\n",
        "    full_model_input = Input(shape=(224, 224, 3), name='full_model_input')\n",
        "\n",
        "    # Get the encoder output (embeddings) for the given input\n",
        "    encoder_output = encoder(full_model_input)\n",
        "\n",
        "    # Feed the encoder output (embeddings) as input to the decoder\n",
        "    decoder_output = decoder(encoder_output)\n",
        "\n",
        "    print(decoder_output.shape)\n",
        "\n",
        "    # Define the full end-to-end model\n",
        "    full_model = Model(inputs=full_model_input, outputs=decoder_output, name='end_to_end_model')\n",
        "\n",
        "    return full_model\n",
        "\n",
        "# Build the full model\n",
        "end_to_end_model = build_end_to_end_model(encoder_model, decoder_model)\n",
        "\n",
        "# Model compilation (adjust optimizer and loss as needed)\n",
        "end_to_end_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Display the model summary to verify the architecture\n",
        "end_to_end_model.summary()\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
        "\n",
        "\n",
        "# def preprocess_text(texts, tokenizer, max_seq_length):\n",
        "#     sequences = tokenizer.texts_to_sequences(texts)\n",
        "#     padded_sequences = pad_sequences(sequences, maxlen=max_seq_length, padding='post')\n",
        "#     return padded_sequences\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size)\n",
        "# Fit your tokenizer here on your text data\n",
        "# print(words)\n",
        "\n",
        "tokenizer.fit_on_texts(words)\n",
        "\n",
        "def convert_to_classes(predictions):\n",
        "    return np.argmax(predictions, axis=-1)\n",
        "\n",
        "# Initialize lists to store precision and recall\n",
        "epoch_precisions = []\n",
        "epoch_recalls = []\n",
        "\n",
        "text_sequences = words\n",
        "\n",
        "\n",
        "# Directory containing preprocessed images\n",
        "image_directory = '/content/drive/My Drive/Dataset/embedded1/'\n",
        "loaded_images = []\n",
        "\n",
        "# Iterate through each file in the directory\n",
        "for filename in os.listdir(image_directory):\n",
        "    if filename.endswith(\".jpg\") or filename.endswith(\".png\"):  # Check for image files\n",
        "        # Construct the full path to the image file\n",
        "        file_path = os.path.join(image_directory, filename)\n",
        "\n",
        "        # Load and preprocess the image\n",
        "        img = cv2.imread(file_path)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        img = img / 255.0  # Normalize image pixels to [0, 1]\n",
        "        loaded_images.append(img)\n",
        "\n",
        "images_array = np.array(loaded_images)\n",
        "\n",
        "text_sequences = tokenizer.texts_to_sequences(text_sequences)\n",
        "target_data = pad_sequences(text_sequences, maxlen=max_seq_length, padding='post')\n",
        "num_samples = len(loaded_images)\n",
        "# One-hot encode your sequences. This might be memory-intensive.\n",
        "target_data_one_hot = np.zeros((num_samples, max_seq_length, vocab_size), dtype='float32')\n",
        "\n",
        "for i, seq in enumerate(target_data):\n",
        "    for j, word_index in enumerate(seq):\n",
        "        if word_index != 0:  # Assuming 0 is used for padding\n",
        "            target_data_one_hot[i, j, word_index] = 1.0\n",
        "\n",
        "# print(images_array.shape)  # Should be (num_samples, 224, 224, 3)\n",
        "print(target_data_one_hot.shape)  # Should be (num_samples, max_seq_length)\n",
        "\n",
        "\n",
        "\n",
        "# Train the model\n",
        "\n",
        "end_to_end_model.fit(images_array, target_data_one_hot, batch_size=32, epochs=10, verbose=1)"
      ]
    }
  ]
}